{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Lesson 9: File I/O & Error Handling\n",
    "\n",
    "**Session:** Week 3, Tuesday (2 hours)  \n",
    "**Learning Objectives:**\n",
    "- Read and write files in Python\n",
    "- Handle different file formats (text, CSV, JSON)\n",
    "- Understand and implement error handling with try/except\n",
    "- Build robust programs that handle unexpected situations\n",
    "- Apply file operations to real data science scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome",
   "metadata": {},
   "source": [
    "## ğŸ‰ Welcome to Week 3! \n",
    "\n",
    "Congratulations on completing Week 2! You now have solid foundations in:\n",
    "- **Decision making** with conditionals\n",
    "- **Repetition** with loops  \n",
    "- **Code organization** with functions\n",
    "\n",
    "This week, we'll learn to work with **real data** and build **production-ready** programs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "week2_recap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Week 2 Skills Check\n",
    "def skills_assessment():\n",
    "    \"\"\"Demonstrate Week 2 mastery in one function\"\"\"\n",
    "    \n",
    "    # Conditionals + Functions\n",
    "    scores = [85, 92, 78, 95, 88]\n",
    "    \n",
    "    # Loops + Conditionals\n",
    "    results = []\n",
    "    for score in scores:\n",
    "        if score >= 90:\n",
    "            grade = \"A\"\n",
    "        elif score >= 80:\n",
    "            grade = \"B\"\n",
    "        else:\n",
    "            grade = \"C\"\n",
    "        results.append(f\"Score {score}: Grade {grade}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test our combined skills\n",
    "assessment_results = skills_assessment()\n",
    "for result in assessment_results:\n",
    "    print(result)\n",
    "    \n",
    "print(\"\\nâœ… Week 2 skills confirmed! Ready for Week 3! ğŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file_intro",
   "metadata": {},
   "source": [
    "## The Problem: Data Lives in Files! ğŸ“\n",
    "\n",
    "So far, we've worked with data we create in our programs. But in the real world:\n",
    "\n",
    "- **Data scientists** work with CSV files, JSON APIs, databases\n",
    "- **Web developers** read configuration files, serve HTML/CSS\n",
    "- **System administrators** process log files, manage configs\n",
    "- **Everyone** needs to save and load information\n",
    "\n",
    "**Today's mission:** Learn to work with files like a pro! ğŸ’ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file_system_analogy",
   "metadata": {},
   "source": [
    "## The File Cabinet Analogy ğŸ—ƒï¸\n",
    "\n",
    "### Think of File Operations Like Office Work\n",
    "\n",
    "**File System = Office Building**\n",
    "- **Folders** = Filing cabinets\n",
    "- **Files** = Documents in folders\n",
    "- **File paths** = Office addresses (\"Building/Floor/Cabinet/Folder\")\n",
    "\n",
    "**File Operations = Office Tasks**\n",
    "- **Opening** a file = Taking a document from the cabinet\n",
    "- **Reading** = Looking at the document's content\n",
    "- **Writing** = Adding or changing content\n",
    "- **Closing** = Putting the document back properly\n",
    "\n",
    "**File Modes = Document Permissions**\n",
    "- **Read ('r')** = \"View only\" - can look but not change\n",
    "- **Write ('w')** = \"Replace entirely\" - throw away old, write new\n",
    "- **Append ('a')** = \"Add to end\" - keep existing, add more\n",
    "\n",
    "Just like in an office, you must **open** before you work and **close** when done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_file_operations",
   "metadata": {},
   "source": [
    "## Basic File Operations ğŸ“–\n",
    "\n",
    "### The Essential Pattern: Open â†’ Work â†’ Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_sample_file",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a sample file to work with\n",
    "sample_content = \"\"\"Welcome to Python File I/O!\n",
    "This is line 2 of our sample file.\n",
    "Line 3 contains some data: Python, Java, JavaScript\n",
    "Final line: Remember to close your files!\"\"\"\n",
    "\n",
    "# Create our sample file\n",
    "with open('sample.txt', 'w') as file:\n",
    "    file.write(sample_content)\n",
    "\n",
    "print(\"âœ… Sample file 'sample.txt' created!\")\n",
    "print(\"ğŸ“ File contains 4 lines of text about Python programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Basic file reading (manual close)\n",
    "print(\"=== Method 1: Manual File Handling ===\")\n",
    "\n",
    "# Step 1: Open the file\n",
    "file = open('sample.txt', 'r')  # 'r' = read mode\n",
    "\n",
    "# Step 2: Read the content\n",
    "content = file.read()  # Read entire file as one string\n",
    "\n",
    "# Step 3: Close the file (IMPORTANT!)\n",
    "file.close()\n",
    "\n",
    "print(\"File content:\")\n",
    "print(content)\n",
    "print(f\"Content type: {type(content)}\")\n",
    "print(f\"Content length: {len(content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "with_statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using 'with' statement (RECOMMENDED!)\n",
    "print(\"=== Method 2: 'with' Statement (Best Practice) ===\")\n",
    "\n",
    "# The 'with' statement automatically closes the file\n",
    "with open('sample.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(\"ğŸ“– Reading file with 'with' statement:\")\n",
    "    print(content)\n",
    "    print(\"\\nğŸ”„ File automatically closes when 'with' block ends!\")\n",
    "\n",
    "# File is automatically closed here - no need to call file.close()\n",
    "print(\"\\nâœ… File handling complete and safe!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reading_methods",
   "metadata": {},
   "source": [
    "## Different Ways to Read Files ğŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reading_methods_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Different File Reading Methods ===\")\n",
    "\n",
    "# Method 1: Read entire file as one string\n",
    "print(\"\\n1. file.read() - Entire file as string:\")\n",
    "with open('sample.txt', 'r') as file:\n",
    "    entire_content = file.read()\n",
    "    print(repr(entire_content))  # repr() shows \\n characters\n",
    "\n",
    "# Method 2: Read file line by line into a list\n",
    "print(\"\\n2. file.readlines() - List of lines:\")\n",
    "with open('sample.txt', 'r') as file:\n",
    "    lines_list = file.readlines()\n",
    "    print(f\"Lines: {lines_list}\")\n",
    "    print(f\"Number of lines: {len(lines_list)}\")\n",
    "\n",
    "# Method 3: Read one line at a time\n",
    "print(\"\\n3. file.readline() - One line at a time:\")\n",
    "with open('sample.txt', 'r') as file:\n",
    "    first_line = file.readline()\n",
    "    second_line = file.readline()\n",
    "    print(f\"First line: {repr(first_line)}\")\n",
    "    print(f\"Second line: {repr(second_line)}\")\n",
    "\n",
    "# Method 4: Iterate through lines (MOST COMMON)\n",
    "print(\"\\n4. for line in file - Iterate through lines:\")\n",
    "with open('sample.txt', 'r') as file:\n",
    "    for line_number, line in enumerate(file, 1):\n",
    "        # strip() removes the \\n at the end of each line\n",
    "        clean_line = line.strip()\n",
    "        print(f\"Line {line_number}: {clean_line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "writing_files",
   "metadata": {},
   "source": [
    "## Writing Files âœï¸\n",
    "\n",
    "### Three Writing Modes: Write, Append, and Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "writing_modes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Mode 1: 'w' - Write (overwrites existing file)\n",
    "print(\"=== Mode 'w': Write (Replace) ===\")\n",
    "\n",
    "grocery_list = [\"apples\", \"bananas\", \"bread\", \"milk\", \"eggs\"]\n",
    "\n",
    "with open('grocery_list.txt', 'w') as file:\n",
    "    file.write(\"My Grocery List\\n\")\n",
    "    file.write(\"===============\\n\")\n",
    "    \n",
    "    for i, item in enumerate(grocery_list, 1):\n",
    "        file.write(f\"{i}. {item.title()}\\n\")\n",
    "    \n",
    "    file.write(\"\\nTotal items: \" + str(len(grocery_list)))\n",
    "\n",
    "print(\"âœ… Grocery list written to 'grocery_list.txt'\")\n",
    "\n",
    "# Read it back to verify\n",
    "with open('grocery_list.txt', 'r') as file:\n",
    "    print(\"\\nğŸ“– File contents:\")\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "append_mode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Mode 2: 'a' - Append (add to existing file)\n",
    "print(\"=== Mode 'a': Append ===\")\n",
    "\n",
    "# Add more items to our grocery list\n",
    "additional_items = [\"cheese\", \"yogurt\", \"chicken\"]\n",
    "\n",
    "with open('grocery_list.txt', 'a') as file:  # 'a' = append mode\n",
    "    file.write(\"\\n\\nAdditional Items:\\n\")\n",
    "    file.write(\"================\\n\")\n",
    "    \n",
    "    for item in additional_items:\n",
    "        file.write(f\"â€¢ {item.title()}\\n\")\n",
    "\n",
    "print(\"âœ… Additional items appended to grocery list\")\n",
    "\n",
    "# Read the updated file\n",
    "with open('grocery_list.txt', 'r') as file:\n",
    "    print(\"\\nğŸ“– Updated file contents:\")\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "writelines_method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced writing: writelines() method\n",
    "print(\"=== Using writelines() for Multiple Lines ===\")\n",
    "\n",
    "# Create a simple log file\n",
    "from datetime import datetime\n",
    "\n",
    "log_entries = [\n",
    "    f\"{datetime.now()}: Program started\\n\",\n",
    "    f\"{datetime.now()}: Processing data\\n\",\n",
    "    f\"{datetime.now()}: Analysis complete\\n\",\n",
    "    f\"{datetime.now()}: Results saved\\n\"\n",
    "]\n",
    "\n",
    "with open('program_log.txt', 'w') as file:\n",
    "    file.write(\"Program Execution Log\\n\")\n",
    "    file.write(\"====================\\n\")\n",
    "    file.writelines(log_entries)  # Write multiple lines at once\n",
    "\n",
    "print(\"âœ… Log file created with writelines()\")\n",
    "\n",
    "# Read and display the log\n",
    "with open('program_log.txt', 'r') as file:\n",
    "    print(\"\\nğŸ“– Log file contents:\")\n",
    "    for line in file:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "live_coding_1",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Live Coding: Student Grade Manager\n",
    "\n",
    "Let's build a complete system that saves and loads student data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grade_manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Grade Management System\n",
    "print(\"=== Student Grade Management System ===\")\n",
    "\n",
    "def save_student_grades(students_data, filename='student_grades.txt'):\n",
    "    \"\"\"\n",
    "    Save student grade data to a text file\n",
    "    \n",
    "    Parameters:\n",
    "    - students_data: dict {student_name: [grades]}\n",
    "    - filename: str (output file name)\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"Student Grade Report\\n\")\n",
    "        file.write(\"==================\\n\\n\")\n",
    "        \n",
    "        for student, grades in students_data.items():\n",
    "            average = sum(grades) / len(grades)\n",
    "            \n",
    "            file.write(f\"Student: {student}\\n\")\n",
    "            file.write(f\"Grades: {', '.join(map(str, grades))}\\n\")\n",
    "            file.write(f\"Average: {average:.2f}\\n\")\n",
    "            \n",
    "            # Determine letter grade\n",
    "            if average >= 90:\n",
    "                letter = 'A'\n",
    "            elif average >= 80:\n",
    "                letter = 'B'\n",
    "            elif average >= 70:\n",
    "                letter = 'C'\n",
    "            else:\n",
    "                letter = 'F'\n",
    "                \n",
    "            file.write(f\"Letter Grade: {letter}\\n\")\n",
    "            file.write(\"-\" * 30 + \"\\n\")\n",
    "    \n",
    "    print(f\"âœ… Student grades saved to '{filename}'\")\n",
    "\n",
    "def load_and_analyze_grades(filename='student_grades.txt'):\n",
    "    \"\"\"\n",
    "    Load and analyze student grades from file\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: str (input file name)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“– Loading grades from '{filename}':\")\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "        print(content)\n",
    "    \n",
    "    # Bonus: Extract and analyze data\n",
    "    averages = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('Average:'):\n",
    "                # Extract the average value\n",
    "                avg_str = line.split(':')[1].strip()\n",
    "                averages.append(float(avg_str))\n",
    "    \n",
    "    if averages:\n",
    "        class_average = sum(averages) / len(averages)\n",
    "        print(f\"\\nğŸ“Š Class Statistics:\")\n",
    "        print(f\"Total students: {len(averages)}\")\n",
    "        print(f\"Class average: {class_average:.2f}\")\n",
    "        print(f\"Highest average: {max(averages):.2f}\")\n",
    "        print(f\"Lowest average: {min(averages):.2f}\")\n",
    "\n",
    "# Sample student data\n",
    "students = {\n",
    "    'Alice Johnson': [95, 88, 92, 90],\n",
    "    'Bob Smith': [78, 85, 80, 77],\n",
    "    'Charlie Brown': [88, 91, 87, 89],\n",
    "    'Diana Prince': [92, 95, 98, 94],\n",
    "    'Eve Wilson': [85, 87, 83, 86]\n",
    "}\n",
    "\n",
    "# Save grades to file\n",
    "save_student_grades(students)\n",
    "\n",
    "# Load and display grades\n",
    "load_and_analyze_grades()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error_handling_intro",
   "metadata": {},
   "source": [
    "## Error Handling: When Things Go Wrong ğŸš¨\n",
    "\n",
    "### The Reality Check\n",
    "In the real world, things go wrong:\n",
    "- Files don't exist\n",
    "- Permissions are denied  \n",
    "- Disk space runs out\n",
    "- Users enter invalid data\n",
    "- Network connections fail\n",
    "\n",
    "**Professional programmers handle errors gracefully!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exception_analogy",
   "metadata": {},
   "source": [
    "## The Exception Handling Analogy ğŸ­\n",
    "\n",
    "### Think of Exceptions Like Emergency Procedures\n",
    "\n",
    "**Normal Code = Daily Routine**\n",
    "- Wake up, shower, eat breakfast, go to work\n",
    "- Everything goes according to plan\n",
    "\n",
    "**Exception = Emergency Situation**\n",
    "- Fire alarm goes off during work\n",
    "- Car breaks down on highway\n",
    "- Restaurant is closed when you arrive\n",
    "\n",
    "**Exception Handling = Emergency Response Plan**\n",
    "- **try:** \"Attempt your normal routine\"\n",
    "- **except:** \"If specific emergency occurs, do this instead\"\n",
    "- **else:** \"If no emergency, do this bonus thing\"\n",
    "- **finally:** \"No matter what happens, always do this\"\n",
    "\n",
    "Good emergency plans keep you safe and functioning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common_errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what happens WITHOUT error handling\n",
    "print(\"=== What Happens When Files Don't Exist ===\")\n",
    "\n",
    "try:\n",
    "    # This will cause an error - file doesn't exist\n",
    "    with open('nonexistent_file.txt', 'r') as file:\n",
    "        content = file.read()\n",
    "        print(content)\nexcept FileNotFoundError as e:\n",
    "    print(f\"âŒ Error occurred: {e}\")\n",
    "    print(\"ğŸ›¡ï¸ But our program didn't crash - we handled it!\")\n",
    "\n",
    "print(\"\\nâœ… Program continues running normally...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "try_except_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic try-except structure\n",
    "print(\"=== Basic Error Handling Structure ===\")\n",
    "\n",
    "def safe_file_reader(filename):\n",
    "    \"\"\"\n",
    "    Safely read a file with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This is what we WANT to do\n",
    "        print(f\"ğŸ” Attempting to read '{filename}'...\")\n",
    "        with open(filename, 'r') as file:\n",
    "            content = file.read()\n",
    "            print(f\"âœ… Successfully read {len(content)} characters\")\n",
    "            return content\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        # Handle the specific case where file doesn't exist\n",
    "        print(f\"âŒ File '{filename}' not found!\")\n",
    "        print(\"ğŸ’¡ Tip: Check the filename and path\")\n",
    "        return None\n",
    "        \n",
    "    except PermissionError:\n",
    "        # Handle the case where we don't have permission\n",
    "        print(f\"ğŸ”’ Permission denied for '{filename}'\")\n",
    "        print(\"ğŸ’¡ Tip: Check file permissions\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected errors\n",
    "        print(f\"âš ï¸ Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with existing file\n",
    "print(\"Test 1: Existing file\")\n",
    "content1 = safe_file_reader('sample.txt')\n",
    "\n",
    "print(\"\\nTest 2: Non-existent file\")\n",
    "content2 = safe_file_reader('missing_file.txt')\n",
    "\n",
    "print(\"\\nâœ… Both tests completed - program didn't crash!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete_error_handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete error handling with try-except-else-finally\n",
    "print(\"=== Complete Error Handling Pattern ===\")\n",
    "\n",
    "def robust_file_processor(filename, backup_filename=None):\n",
    "    \"\"\"\n",
    "    Process a file with comprehensive error handling\n",
    "    \"\"\"\n",
    "    processed_lines = 0\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ”„ Processing '{filename}'...\")\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                # Simulate processing each line\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    processed_lines += 1\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ '{filename}' not found!\")\n",
    "        \n",
    "        if backup_filename:\n",
    "            print(f\"ğŸ”„ Trying backup file '{backup_filename}'...\")\n",
    "            return robust_file_processor(backup_filename)  # Recursive call\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "    except PermissionError:\n",
    "        print(f\"ğŸ”’ No permission to read '{filename}'\")\n",
    "        return 0\n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"ğŸ“ '{filename}' has encoding issues\")\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Unexpected error: {type(e).__name__}: {e}\")\n",
    "        return 0\n",
    "        \n",
    "    else:\n",
    "        # This runs only if NO exception occurred\n",
    "        print(f\"âœ… Processing completed successfully!\")\n",
    "        \n",
    "    finally:\n",
    "        # This ALWAYS runs, regardless of success or failure\n",
    "        print(f\"ğŸ“Š Final report: {processed_lines} lines processed\")\n",
    "        \n",
    "    return processed_lines\n",
    "\n",
    "# Test the robust processor\n",
    "print(\"Test 1: Process existing file\")\n",
    "result1 = robust_file_processor('sample.txt')\n",
    "\n",
    "print(\"\\nTest 2: Process missing file with backup\")\n",
    "result2 = robust_file_processor('missing.txt', 'sample.txt')\n",
    "\n",
    "print(\"\\nTest 3: Process missing file without backup\")\n",
    "result3 = robust_file_processor('missing.txt')\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Results: {result1}, {result2}, {result3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csv_files",
   "metadata": {},
   "source": [
    "## Working with CSV Files ğŸ“Š\n",
    "\n",
    "**CSV (Comma-Separated Values)** files are everywhere in data science!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csv_manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Manual CSV handling (educational purpose)\n",
    "print(\"=== Manual CSV Processing ===\")\n",
    "\n",
    "# Create sample sales data\n",
    "sales_data = [\n",
    "    ['Date', 'Product', 'Quantity', 'Price', 'Total'],\n",
    "    ['2024-01-15', 'Laptop', '2', '999.99', '1999.98'],\n",
    "    ['2024-01-16', 'Mouse', '5', '29.99', '149.95'],\n",
    "    ['2024-01-17', 'Keyboard', '3', '89.99', '269.97'],\n",
    "    ['2024-01-18', 'Monitor', '1', '299.99', '299.99'],\n",
    "    ['2024-01-19', 'Headphones', '4', '149.99', '599.96']\n",
    "]\n",
    "\n",
    "# Write CSV manually\n",
    "with open('sales_data.csv', 'w') as file:\n",
    "    for row in sales_data:\n",
    "        # Join items with commas and add newline\n",
    "        csv_line = ','.join(row) + '\\n'\n",
    "        file.write(csv_line)\n",
    "\n",
    "print(\"âœ… CSV file created manually\")\n",
    "\n",
    "# Read CSV manually\n",
    "print(\"\\nğŸ“– Reading CSV file manually:\")\n",
    "with open('sales_data.csv', 'r') as file:\n",
    "    for line_num, line in enumerate(file, 1):\n",
    "        # Split by comma and remove whitespace\n",
    "        columns = [col.strip() for col in line.split(',')]\n",
    "        print(f\"Row {line_num}: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csv_module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using Python's csv module (RECOMMENDED)\n",
    "import csv\n",
    "\n",
    "print(\"=== Professional CSV Handling with csv Module ===\")\n",
    "\n",
    "# Write CSV with csv module\n",
    "employee_data = [\n",
    "    {'Name': 'Alice Johnson', 'Department': 'Engineering', 'Salary': 95000, 'Years': 3},\n",
    "    {'Name': 'Bob Smith', 'Department': 'Marketing', 'Salary': 72000, 'Years': 2},\n",
    "    {'Name': 'Charlie Brown', 'Department': 'Engineering', 'Salary': 88000, 'Years': 4},\n",
    "    {'Name': 'Diana Prince', 'Department': 'HR', 'Salary': 78000, 'Years': 5},\n",
    "    {'Name': 'Eve Wilson', 'Department': 'Engineering', 'Salary': 102000, 'Years': 6}\n",
    "]\n",
    "\n",
    "# Write using DictWriter\n",
    "with open('employees.csv', 'w', newline='') as file:\n",
    "    fieldnames = ['Name', 'Department', 'Salary', 'Years']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write header row\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write data rows\n",
    "    writer.writerows(employee_data)\n",
    "\n",
    "print(\"âœ… Employee CSV created with csv module\")\n",
    "\n",
    "# Read using DictReader\n",
    "print(\"\\nğŸ“– Reading CSV with DictReader:\")\n",
    "try:\n",
    "    with open('employees.csv', 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        print(f\"Columns: {reader.fieldnames}\")\n",
    "        print()\n",
    "        \n",
    "        for row_num, row in enumerate(reader, 1):\n",
    "            print(f\"Employee {row_num}:\")\n",
    "            print(f\"  Name: {row['Name']}\")\n",
    "            print(f\"  Department: {row['Department']}\")\n",
    "            print(f\"  Salary: ${int(row['Salary']):,}\")\n",
    "            print(f\"  Years: {row['Years']}\")\n",
    "            print()\n",
    "            \nexcept FileNotFoundError:\n",
    "    print(\"âŒ Employee CSV file not found!\")\nexcept Exception as e:\n",
    "    print(f\"âš ï¸ Error reading CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csv_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Data Analysis Example\n",
    "print(\"=== CSV Data Analysis Example ===\")\n",
    "\n",
    "def analyze_employee_data(filename='employees.csv'):\n",
    "    \"\"\"\n",
    "    Analyze employee data from CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        employees = []\n",
    "        \n",
    "        # Read all employee data\n",
    "        with open(filename, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            \n",
    "            for row in reader:\n",
    "                # Convert numeric fields\n",
    "                row['Salary'] = int(row['Salary'])\n",
    "                row['Years'] = int(row['Years'])\n",
    "                employees.append(row)\n",
    "        \n",
    "        if not employees:\n",
    "            print(\"âš ï¸ No employee data found!\")\n",
    "            return\n",
    "        \n",
    "        # Analysis\n",
    "        total_employees = len(employees)\n",
    "        salaries = [emp['Salary'] for emp in employees]\n",
    "        years_list = [emp['Years'] for emp in employees]\n",
    "        \n",
    "        # Department analysis\n",
    "        departments = {}\n",
    "        for emp in employees:\n",
    "            dept = emp['Department']\n",
    "            if dept not in departments:\n",
    "                departments[dept] = []\n",
    "            departments[dept].append(emp)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"ğŸ“Š Employee Analysis Results:\")\n",
    "        print(f\"Total Employees: {total_employees}\")\n",
    "        print(f\"Average Salary: ${sum(salaries) / len(salaries):,.2f}\")\n",
    "        print(f\"Salary Range: ${min(salaries):,} - ${max(salaries):,}\")\n",
    "        print(f\"Average Years: {sum(years_list) / len(years_list):.1f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¢ Department Breakdown:\")\n",
    "        for dept, emp_list in departments.items():\n",
    "            dept_salaries = [emp['Salary'] for emp in emp_list]\n",
    "            avg_salary = sum(dept_salaries) / len(dept_salaries)\n",
    "            print(f\"  {dept}: {len(emp_list)} employees, avg salary: ${avg_salary:,.2f}\")\n",
    "        \n",
    "        # Find highest paid employee\n",
    "        highest_paid = max(employees, key=lambda emp: emp['Salary'])\n",
    "        print(f\"\\nğŸ’° Highest Paid: {highest_paid['Name']} (${highest_paid['Salary']:,})\")\n",
    "        \n",
    "        # Find most experienced\n",
    "        most_experienced = max(employees, key=lambda emp: emp['Years'])\n",
    "        print(f\"ğŸ† Most Experienced: {most_experienced['Name']} ({most_experienced['Years']} years)\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File '{filename}' not found!\")\n",
    "    except csv.Error as e:\n",
    "        print(f\"ğŸ“„ CSV error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Unexpected error: {e}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_employee_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json_files",
   "metadata": {},
   "source": [
    "## Working with JSON Files ğŸ“„\n",
    "\n",
    "**JSON (JavaScript Object Notation)** is perfect for structured data and APIs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json_basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\nfrom datetime import datetime\n\nprint(\"=== JSON File Operations ===\")\n\n# Create complex Python data structure\ncourse_data = {\n    \"course_info\": {\n        \"title\": \"Python Fundamentals for Data Science\",\n        \"instructor\": \"Dr. Python Expert\",\n        \"duration_weeks\": 3,\n        \"start_date\": \"2024-01-15\",\n        \"format\": \"online\"\n    },\n    \"students\": [\n        {\n            \"id\": 1,\n            \"name\": \"Alice Johnson\",\n            \"email\": \"alice@email.com\",\n            \"assignments\": {\n                \"week1\": {\"completed\": True, \"score\": 95},\n                \"week2\": {\"completed\": True, \"score\": 88},\n                \"week3\": {\"completed\": False, \"score\": None}\n            },\n            \"attendance\": [\"2024-01-15\", \"2024-01-17\", \"2024-01-22\"]\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Bob Smith\",\n            \"email\": \"bob@email.com\",\n            \"assignments\": {\n                \"week1\": {\"completed\": True, \"score\": 82},\n                \"week2\": {\"completed\": True, \"score\": 79},\n                \"week3\": {\"completed\": False, \"score\": None}\n            },\n            \"attendance\": [\"2024-01-15\", \"2024-01-22\"]\n        }\n    ],\n    \"statistics\": {\n        \"total_enrolled\": 2,\n        \"avg_attendance_rate\": 0.75,\n        \"completion_rate\": {\n            \"week1\": 1.0,\n            \"week2\": 1.0,\n            \"week3\": 0.0\n        }\n    }\n}\n\nprint(\"ğŸ“Š Created complex course data structure\")\nprint(f\"Course: {course_data['course_info']['title']}\")\nprint(f\"Students: {course_data['statistics']['total_enrolled']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json_write_read",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write JSON to file\n",
    "print(\"\\n=== Writing JSON File ===\")\n",
    "\n",
    "try:\n",
    "    with open('course_data.json', 'w') as file:\n",
    "        json.dump(course_data, file, indent=2)  # indent=2 makes it readable\n",
    "    \n",
    "    print(\"âœ… Course data saved to 'course_data.json'\")\n",
    "    \n",
    "    # Read JSON from file\n",
    "    print(\"\\nğŸ“– Reading JSON File:\")\n",
    "    \n",
    "    with open('course_data.json', 'r') as file:\n",
    "        loaded_data = json.load(file)\n",
    "    \n",
    "    print(\"âœ… JSON data loaded successfully\")\n",
    "    print(f\"Data type: {type(loaded_data)}\")\n",
    "    print(f\"Keys: {list(loaded_data.keys())}\")\n",
    "    \n",
    "    # Access nested data\n",
    "    course_title = loaded_data['course_info']['title']\n",
    "    first_student = loaded_data['students'][0]['name']\n",
    "    week1_score = loaded_data['students'][0]['assignments']['week1']['score']\n",
    "    \n",
    "    print(f\"\\nğŸ“ Course: {course_title}\")\n",
    "    print(f\"ğŸ‘¨â€ğŸ“ First student: {first_student}\")\n",
    "    print(f\"ğŸ“ Their Week 1 score: {week1_score}\")\n",
    "    \nexcept json.JSONDecodeError as e:\n    print(f\"ğŸ“„ JSON format error: {e}\")\nexcept FileNotFoundError:\n    print(\"âŒ JSON file not found!\")\nexcept Exception as e:\n    print(f\"âš ï¸ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Data Analysis and Modification\n",
    "print(\"=== JSON Data Analysis and Updates ===\")\n",
    "\n",
    "def analyze_and_update_course_data(filename='course_data.json'):\n",
    "    \"\"\"\n",
    "    Analyze course data and add new information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load existing data\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        print(\"ğŸ“Š Course Data Analysis:\")\n",
    "        \n",
    "        # Analyze student performance\n",
    "        students = data['students']\n",
    "        total_students = len(students)\n",
    "        \n",
    "        print(f\"Total Students: {total_students}\")\n",
    "        \n",
    "        # Calculate average scores by week\n",
    "        week_scores = {'week1': [], 'week2': [], 'week3': []}\n",
    "        \n",
    "        for student in students:\n",
    "            assignments = student['assignments']\n",
    "            for week, assignment in assignments.items():\n",
    "                if assignment['completed'] and assignment['score']:\n",
    "                    week_scores[week].append(assignment['score'])\n",
    "        \n",
    "        print(\"\\nğŸ“ˆ Weekly Average Scores:\")\n",
    "        for week, scores in week_scores.items():\n",
    "            if scores:\n",
    "                avg = sum(scores) / len(scores)\n",
    "                print(f\"  {week.title()}: {avg:.1f} (from {len(scores)} students)\")\n",
    "            else:\n",
    "                print(f\"  {week.title()}: No completed assignments yet\")\n",
    "        \n",
    "        # Add analysis results to the data\n",
    "        current_time = datetime.now().isoformat()\n",
    "        data['last_analysis'] = current_time\n",
    "        data['analysis_results'] = {\n",
    "            'weekly_averages': {\n",
    "                week: sum(scores)/len(scores) if scores else 0 \n",
    "                for week, scores in week_scores.items()\n",
    "            },\n",
    "            'completion_status': {\n",
    "                week: len(scores) for week, scores in week_scores.items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Find student with highest average\n",
    "        best_student = None\n",
    "        highest_avg = 0\n",
    "        \n",
    "        for student in students:\n",
    "            completed_scores = [\n",
    "                assignment['score'] \n",
    "                for assignment in student['assignments'].values() \n",
    "                if assignment['completed'] and assignment['score']\n",
    "            ]\n",
    "            \n",
    "            if completed_scores:\n",
    "                avg = sum(completed_scores) / len(completed_scores)\n",
    "                if avg > highest_avg:\n",
    "                    highest_avg = avg\n",
    "                    best_student = student['name']\n",
    "        \n",
    "        if best_student:\n",
    "            print(f\"\\nğŸ† Top Student: {best_student} (Average: {highest_avg:.1f})\")\n",
    "            data['analysis_results']['top_student'] = {\n",
    "                'name': best_student,\n",
    "                'average': round(highest_avg, 1)\n",
    "            }\n",
    "        \n",
    "        # Save updated data\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data, file, indent=2)\n",
    "        \n",
    "        print(f\"\\nâœ… Analysis complete! Data updated in '{filename}'\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File '{filename}' not found!\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"ğŸ“„ JSON error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error: {e}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_and_update_course_data()\n",
    "\n",
    "# Show the updated file content\n",
    "print(\"\\nğŸ“– Updated JSON structure:\")\n",
    "try:\n",
    "    with open('course_data.json', 'r') as file:\n",
    "        updated_data = json.load(file)\n",
    "    \n",
    "    print(f\"New keys added: {[k for k in updated_data.keys() if k not in course_data.keys()]}\")\n",
    "    print(f\"Analysis timestamp: {updated_data.get('last_analysis', 'Not found')}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"Error reading updated file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_section",
   "metadata": {},
   "source": [
    "## ğŸ¯ In-Class Exercise: Data Processing Pipeline (25 minutes)\n",
    "\n",
    "Build a complete data processing system with error handling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Build a Personal Finance Tracker\n",
    "print(\"ğŸ’° Personal Finance Tracker Exercise\")\n",
    "print(\"Build a system that processes financial transactions!\")\n",
    "\n",
    "# TODO: Create sample transaction data\n",
    "sample_transactions = [\n",
    "    {'date': '2024-01-15', 'category': 'Food', 'amount': -45.67, 'description': 'Grocery shopping'},\n",
    "    {'date': '2024-01-16', 'category': 'Income', 'amount': 3000.00, 'description': 'Salary'},\n",
    "    {'date': '2024-01-17', 'category': 'Transportation', 'amount': -28.50, 'description': 'Gas'},\n",
    "    {'date': '2024-01-18', 'category': 'Entertainment', 'amount': -15.99, 'description': 'Netflix'},\n",
    "    {'date': '2024-01-19', 'category': 'Food', 'amount': -32.40, 'description': 'Restaurant'},\n",
    "]\n",
    "\n",
    "def save_transactions_json(transactions, filename='transactions.json'):\n",
    "    \"\"\"\n",
    "    TODO: Save transactions to JSON file with error handling\n",
    "    \n",
    "    Requirements:\n",
    "    1. Use try-except for error handling\n",
    "    2. Add metadata (timestamp, total transactions)\n",
    "    3. Format JSON nicely with indentation\n",
    "    4. Return success/failure status\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def load_and_analyze_transactions(filename='transactions.json'):\n",
    "    \"\"\"\n",
    "    TODO: Load transactions and perform analysis\n",
    "    \n",
    "    Requirements:\n",
    "    1. Handle file not found gracefully\n",
    "    2. Calculate total income, expenses, net\n",
    "    3. Group by category\n",
    "    4. Find largest expense\n",
    "    5. Generate summary report\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def export_summary_csv(transactions, filename='financial_summary.csv'):\n",
    "    \"\"\"\n",
    "    TODO: Export category summaries to CSV\n",
    "    \n",
    "    Requirements:\n",
    "    1. Group transactions by category\n",
    "    2. Calculate totals and averages per category\n",
    "    3. Export to CSV with proper headers\n",
    "    4. Handle write errors\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "print(\"Testing finance tracker...\")\n",
    "\n",
    "# Save sample data\n",
    "# save_result = save_transactions_json(sample_transactions)\n",
    "\n",
    "# Load and analyze\n",
    "# analysis = load_and_analyze_transactions()\n",
    "\n",
    "# Export summary\n",
    "# export_result = export_summary_csv(sample_transactions)\n",
    "\n",
    "print(\"\\nğŸ¯ Complete the functions above to finish the exercise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "## File I/O Best Practices ğŸŒŸ\n",
    "\n",
    "### Professional Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_practices_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices Demonstration\nimport os\nfrom pathlib import Path\n\nprint(\"=== File I/O Best Practices ===\")\n\n# Practice 1: Always use 'with' statements\ndef good_file_handling(filename):\n    \"\"\"Good: Automatic file closing\"\"\"\n    try:\n        with open(filename, 'r') as file:\n            return file.read()\n    except FileNotFoundError:\n        return None\n\n# Practice 2: Check if files exist before processing\ndef safe_file_check(filename):\n    \"\"\"Check file existence safely\"\"\"\n    if not os.path.exists(filename):\n        print(f\"âŒ File '{filename}' does not exist\")\n        return False\n    \n    if not os.path.isfile(filename):\n        print(f\"âŒ '{filename}' is not a regular file\")\n        return False\n        \n    return True\n\n# Practice 3: Use pathlib for cross-platform paths\nfrom pathlib import Path\n\ndef modern_path_handling():\n    \"\"\"Modern path handling with pathlib\"\"\"\n    # Create paths that work on Windows, Mac, and Linux\n    data_dir = Path('data')\n    csv_file = data_dir / 'employees.csv'\n    json_file = data_dir / 'course_data.json'\n    \n    print(f\"Data directory: {data_dir}\")\n    print(f\"CSV file path: {csv_file}\")\n    print(f\"JSON file path: {json_file}\")\n    \n    # Check if path exists\n    if csv_file.exists():\n        print(f\"âœ… {csv_file.name} exists\")\n    else:\n        print(f\"âŒ {csv_file.name} not found\")\n\n# Practice 4: Validate file contents\ndef validate_csv_structure(filename, expected_columns):\n    \"\"\"\n    Validate CSV file structure\n    \"\"\"\n    try:\n        import csv\n        with open(filename, 'r') as file:\n            reader = csv.DictReader(file)\n            \n            # Check if expected columns exist\n            missing_columns = set(expected_columns) - set(reader.fieldnames)\n            if missing_columns:\n                print(f\"âŒ Missing columns: {missing_columns}\")\n                return False\n                \n            # Check if file has data\n            row_count = sum(1 for row in reader)\n            if row_count == 0:\n                print(\"âš ï¸ CSV file is empty\")\n                return False\n                \n            print(f\"âœ… CSV valid: {row_count} rows, all columns present\")\n            return True\n            \n    except Exception as e:\n        print(f\"âŒ CSV validation failed: {e}\")\n        return False\n\n# Practice 5: Create backup files\ndef create_backup(filename):\n    \"\"\"\n    Create a backup copy before modifying\n    \"\"\"\n    if not os.path.exists(filename):\n        return False\n        \n    backup_name = filename + '.backup'\n    try:\n        import shutil\n        shutil.copy2(filename, backup_name)\n        print(f\"âœ… Backup created: {backup_name}\")\n        return True\n    except Exception as e:\n        print(f\"âŒ Backup failed: {e}\")\n        return False\n\n# Demonstrate best practices\nprint(\"\\n1. Path handling:\")\nmodern_path_handling()\n\nprint(\"\\n2. File validation:\")\nif safe_file_check('employees.csv'):\n    validate_csv_structure('employees.csv', ['Name', 'Department', 'Salary', 'Years'])\n\nprint(\"\\n3. Backup creation:\")\nif os.path.exists('employees.csv'):\n    create_backup('employees.csv')\n\nprint(\"\\nğŸ“‹ Best Practices Summary:\")\nprint(\"âœ… Always use 'with' statements\")\nprint(\"âœ… Handle exceptions appropriately\")\nprint(\"âœ… Validate file existence and structure\")\nprint(\"âœ… Use pathlib for cross-platform compatibility\")\nprint(\"âœ… Create backups before modifying files\")\nprint(\"âœ… Close files properly (automatic with 'with')\")\nprint(\"âœ… Use appropriate file modes ('r', 'w', 'a')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real_world_applications",
   "metadata": {},
   "source": [
    "## Real-World Applications ğŸŒ\n",
    "\n",
    "### How Data Scientists Use File I/O Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_science_scenarios",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world data science scenarios\nprint(\"=== Data Science File I/O Scenarios ===\")\n\n# Scenario 1: Processing sensor data\ndef process_sensor_data():\n    \"\"\"\n    Simulate processing IoT sensor data\n    \"\"\"\n    print(\"\\nğŸŒ¡ï¸ Scenario 1: IoT Sensor Data Processing\")\n    \n    # Simulate sensor data\n    import random\n    from datetime import datetime, timedelta\n    \n    sensor_data = []\n    base_time = datetime.now()\n    \n    for i in range(24):  # 24 hours of data\n        timestamp = base_time + timedelta(hours=i)\n        temperature = round(random.uniform(18, 28), 1)\n        humidity = round(random.uniform(40, 80), 1)\n        \n        sensor_data.append({\n            'timestamp': timestamp.isoformat(),\n            'temperature': temperature,\n            'humidity': humidity,\n            'sensor_id': 'SENSOR_001'\n        })\n    \n    # Save raw data\n    try:\n        with open('sensor_data.json', 'w') as file:\n            json.dump(sensor_data, file, indent=2)\n        \n        # Process and create summary\n        temperatures = [reading['temperature'] for reading in sensor_data]\n        humidity_values = [reading['humidity'] for reading in sensor_data]\n        \n        summary = {\n            'period': '24_hours',\n            'readings_count': len(sensor_data),\n            'temperature': {\n                'min': min(temperatures),\n                'max': max(temperatures),\n                'avg': round(sum(temperatures) / len(temperatures), 1)\n            },\n            'humidity': {\n                'min': min(humidity_values),\n                'max': max(humidity_values),\n                'avg': round(sum(humidity_values) / len(humidity_values), 1)\n            },\n            'alerts': []\n        }\n        \n        # Add alerts for extreme values\n        for reading in sensor_data:\n            if reading['temperature'] > 25:\n                summary['alerts'].append(f\"High temp: {reading['temperature']}Â°C at {reading['timestamp'][:16]}\")\n        \n        # Save summary\n        with open('sensor_summary.json', 'w') as file:\n            json.dump(summary, file, indent=2)\n            \n        print(f\"âœ… Processed {len(sensor_data)} sensor readings\")\n        print(f\"ğŸ“Š Temperature range: {summary['temperature']['min']}Â°C - {summary['temperature']['max']}Â°C\")\n        print(f\"âš ï¸ Alerts generated: {len(summary['alerts'])}\")\n        \n    except Exception as e:\n        print(f\"âŒ Error processing sensor data: {e}\")\n\n# Scenario 2: Log file analysis\ndef analyze_web_logs():\n    \"\"\"\n    Simulate web server log analysis\n    \"\"\"\n    print(\"\\nğŸŒ Scenario 2: Web Server Log Analysis\")\n    \n    # Create sample log file\n    log_entries = [\n        \"2024-01-15 10:30:15 GET /api/users 200 0.123\",\n        \"2024-01-15 10:30:16 POST /api/login 401 0.056\",\n        \"2024-01-15 10:30:17 GET /api/dashboard 200 0.234\",\n        \"2024-01-15 10:30:18 GET /static/css/main.css 200 0.012\",\n        \"2024-01-15 10:30:19 POST /api/data 500 1.234\",\n        \"2024-01-15 10:30:20 GET /api/users 200 0.087\",\n    ]\n    \n    try:\n        # Write log file\n        with open('server.log', 'w') as file:\n            for entry in log_entries:\n                file.write(entry + '\\n')\n        \n        # Analyze logs\n        status_counts = {}\n        response_times = []\n        error_logs = []\n        \n        with open('server.log', 'r') as file:\n            for line in file:\n                parts = line.strip().split()\n                if len(parts) >= 5:\n                    method = parts[2]\n                    endpoint = parts[3]\n                    status = parts[4]\n                    response_time = float(parts[5])\n                    \n                    # Count status codes\n                    status_counts[status] = status_counts.get(status, 0) + 1\n                    \n                    # Collect response times\n                    response_times.append(response_time)\n                    \n                    # Log errors\n                    if status.startswith('4') or status.startswith('5'):\n                        error_logs.append(line.strip())\n        \n        # Generate report\n        avg_response_time = sum(response_times) / len(response_times)\n        \n        report = {\n            'total_requests': len(response_times),\n            'status_code_summary': status_counts,\n            'average_response_time': round(avg_response_time, 3),\n            'error_count': len(error_logs),\n            'errors': error_logs\n        }\n        \n        # Save report\n        with open('log_analysis_report.json', 'w') as file:\n            json.dump(report, file, indent=2)\n        \n        print(f\"âœ… Analyzed {report['total_requests']} requests\")\n        print(f\"ğŸ“ˆ Average response time: {report['average_response_time']}s\")\n        print(f\"âŒ Errors found: {report['error_count']}\")\n        \n    except Exception as e:\n        print(f\"âŒ Error analyzing logs: {e}\")\n\n# Scenario 3: Configuration management\ndef manage_config_files():\n    \"\"\"\n    Demonstrate configuration file management\n    \"\"\"\n    print(\"\\nâš™ï¸ Scenario 3: Configuration Management\")\n    \n    # Create application config\n    config = {\n        \"database\": {\n            \"host\": \"localhost\",\n            \"port\": 5432,\n            \"name\": \"analytics_db\",\n            \"ssl_enabled\": True\n        },\n        \"api\": {\n            \"rate_limit\": 1000,\n            \"timeout\": 30,\n            \"debug_mode\": False\n        },\n        \"logging\": {\n            \"level\": \"INFO\",\n            \"file\": \"app.log\",\n            \"max_size_mb\": 100\n        }\n    }\n    \n    try:\n        # Save configuration\n        with open('app_config.json', 'w') as file:\n            json.dump(config, file, indent=2)\n        \n        # Load and validate configuration\n        with open('app_config.json', 'r') as file:\n            loaded_config = json.load(file)\n        \n        # Validate required settings\n        required_sections = ['database', 'api', 'logging']\n        missing_sections = [section for section in required_sections \n                          if section not in loaded_config]\n        \n        if missing_sections:\n            print(f\"âŒ Missing config sections: {missing_sections}\")\n        else:\n            print(\"âœ… Configuration loaded and validated\")\n            print(f\"ğŸ—„ï¸ Database: {loaded_config['database']['host']}:{loaded_config['database']['port']}\")\n            print(f\"ğŸš€ API rate limit: {loaded_config['api']['rate_limit']} requests/hour\")\n        \n    except Exception as e:\n        print(f\"âŒ Config management error: {e}\")\n\n# Run all scenarios\nprocess_sensor_data()\nanalyze_web_logs()\nmanage_config_files()\n\nprint(\"\\nğŸ¯ These scenarios show how file I/O is essential for:\")\nprint(\"â€¢ Processing sensor/IoT data\")\nprint(\"â€¢ Analyzing application logs\")\nprint(\"â€¢ Managing configuration files\")\nprint(\"â€¢ Creating data processing pipelines\")\nprint(\"â€¢ Building ETL (Extract, Transform, Load) systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## ğŸ“š Session Summary\n",
    "\n",
    "ğŸ‰ **Outstanding!** You've mastered file operations and error handling - essential skills for data science!\n",
    "\n",
    "### âœ… Core Skills Acquired\n",
    "- **File Operations**: Read, write, append with proper file handling\n",
    "- **Error Handling**: Graceful exception handling with try/except/finally\n",
    "- **CSV Processing**: Reading and writing structured data for analysis\n",
    "- **JSON Operations**: Working with complex data structures and APIs\n",
    "- **Best Practices**: Professional-grade file handling techniques\n",
    "\n",
    "### ğŸ”‘ Key Patterns Mastered\n",
    "1. **'with' statement**: Automatic file closing and resource management\n",
    "2. **Exception handling**: try/except/else/finally for robust programs\n",
    "3. **Data validation**: Checking file structure and content integrity\n",
    "4. **Path handling**: Cross-platform file path management\n",
    "5. **Data transformation**: Converting between formats (CSV â†” JSON â†” Python objects)\n",
    "\n",
    "### ğŸ—ƒï¸ Remember: The File Cabinet Analogy\n",
    "- **Files** are documents in your office filing system\n",
    "- **Opening** files is like taking documents from cabinets\n",
    "- **Reading/Writing** is like reviewing or updating documents\n",
    "- **Closing** properly is like returning documents to their place\n",
    "- **Error handling** is like having procedures for missing or damaged files\n",
    "\n",
    "### ğŸ  Homework Preview\n",
    "This week's homework will include:\n",
    "1. Building a complete data processing pipeline\n",
    "2. Creating robust error handling systems\n",
    "3. Working with real CSV and JSON datasets\n",
    "4. Implementing data validation and backup systems\n",
    "\n",
    "### ğŸš€ Next Session Preview\n",
    "Thursday we'll learn about **Working with Data** - pandas basics, data cleaning, and analysis techniques!\n",
    "\n",
    "### ğŸ’¡ Pro Tips for Success\n",
    "- Always use `with` statements for file operations\n",
    "- Plan for errors - they WILL happen in real projects\n",
    "- Validate your data before processing\n",
    "- Keep backups of important files\n",
    "- Test with edge cases (empty files, missing files, corrupt data)\n",
    "\n",
    "**You're now ready to handle real-world data like a professional!** ğŸ“Šâœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}