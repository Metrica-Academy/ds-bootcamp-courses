{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Final Capstone Project: Data Science Challenge\n",
    "\n",
    "**Duration:** 1 week (self-paced)  \n",
    "**Deadline:** [To be announced by instructor]  \n",
    "**Weight:** 40% of final grade  \n",
    "**Format:** Individual project with instructor support\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This capstone project is your opportunity to demonstrate mastery of all Python fundamentals by solving a real-world business problem with data. You'll build a complete data analysis system from scratch, showcasing the skills you've developed over the past 3 weeks.\n",
    "\n",
    "### üèÜ Success Criteria\n",
    "- **Technical Excellence**: Clean, well-organized code using all major Python concepts\n",
    "- **Business Impact**: Actionable insights that drive real decision-making\n",
    "- **Professional Quality**: Portfolio-ready work suitable for job applications\n",
    "- **Communication Skills**: Clear presentation of findings to stakeholders\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project_themes",
   "metadata": {},
   "source": [
    "## üé® Project Themes (Choose One)\n",
    "\n",
    "Select the theme that most interests you - passion drives excellence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthcare_theme",
   "metadata": {},
   "source": [
    "### üè• Theme A: Healthcare Analytics - Patient Outcome Optimization\n",
    "\n",
    "**Business Context**: A regional hospital network wants to improve patient outcomes while reducing costs. They need data-driven insights to optimize treatment protocols, resource allocation, and patient care pathways.\n",
    "\n",
    "**Your Role**: Senior Data Analyst reporting to the Chief Medical Officer\n",
    "\n",
    "**Data Sources**:\n",
    "- Patient demographics and medical history\n",
    "- Treatment protocols and medication records\n",
    "- Hospital resource utilization (beds, staff, equipment)\n",
    "- Patient satisfaction surveys and outcome metrics\n",
    "- Financial data (costs, insurance, billing)\n",
    "\n",
    "**Key Business Questions**:\n",
    "1. Which treatments have the best outcome-to-cost ratios?\n",
    "2. Can we predict which patients are at risk for readmission?\n",
    "3. How can we optimize staff scheduling and resource allocation?\n",
    "4. What factors most influence patient satisfaction scores?\n",
    "5. Where are the opportunities for cost reduction without impacting care quality?\n",
    "\n",
    "**Expected Deliverables**:\n",
    "- Patient risk assessment model\n",
    "- Treatment effectiveness analysis\n",
    "- Resource optimization recommendations\n",
    "- Cost-benefit analysis with ROI projections\n",
    "- Executive dashboard data for hospital leadership\n",
    "\n",
    "**Skills Emphasized**:\n",
    "- Data cleaning with medical data complexities\n",
    "- Statistical analysis for healthcare metrics\n",
    "- Risk modeling and prediction\n",
    "- Ethical considerations in healthcare data\n",
    "- Regulatory compliance awareness (HIPAA considerations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finance_theme",
   "metadata": {},
   "source": [
    "### üí∞ Theme B: Financial Services - Credit Risk & Fraud Detection\n",
    "\n",
    "**Business Context**: A growing fintech company needs to improve their credit decision-making and fraud detection capabilities. They want to reduce default rates while expanding access to credit for underserved populations.\n",
    "\n",
    "**Your Role**: Risk Analytics Specialist reporting to the Chief Risk Officer\n",
    "\n",
    "**Data Sources**:\n",
    "- Credit application data and financial history\n",
    "- Transaction patterns and account behavior\n",
    "- Demographic and employment information\n",
    "- External credit bureau scores and reports\n",
    "- Fraud indicators and suspicious activity reports\n",
    "\n",
    "**Key Business Questions**:\n",
    "1. What factors best predict credit default risk?\n",
    "2. Can we identify fraudulent transactions in real-time?\n",
    "3. How can we expand credit access while maintaining risk standards?\n",
    "4. What are the optimal credit limits for different customer segments?\n",
    "5. Which early warning signs predict account deterioration?\n",
    "\n",
    "**Expected Deliverables**:\n",
    "- Credit scoring model with validation\n",
    "- Fraud detection rules and algorithms\n",
    "- Customer segmentation strategy\n",
    "- Risk-adjusted pricing recommendations\n",
    "- Regulatory compliance report\n",
    "\n",
    "**Skills Emphasized**:\n",
    "- Financial data analysis and modeling\n",
    "- Pattern recognition and anomaly detection\n",
    "- Risk quantification and management\n",
    "- Regulatory compliance considerations\n",
    "- Real-time processing system design\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retail_theme",
   "metadata": {},
   "source": [
    "### üõí Theme C: Retail Intelligence - Customer Experience Optimization\n",
    "\n",
    "**Business Context**: A multi-channel retail company wants to enhance customer experience and drive revenue growth. They need insights into customer behavior, inventory optimization, and personalization strategies.\n",
    "\n",
    "**Your Role**: Customer Analytics Manager reporting to the VP of Marketing\n",
    "\n",
    "**Data Sources**:\n",
    "- Customer purchase history and transaction data\n",
    "- Website and mobile app interaction logs\n",
    "- Inventory levels and product performance\n",
    "- Customer service interactions and reviews\n",
    "- Marketing campaign performance and attribution\n",
    "\n",
    "**Key Business Questions**:\n",
    "1. What are the distinct customer segments and their behaviors?\n",
    "2. Which products should be promoted to which customers?\n",
    "3. How can we optimize inventory to reduce stockouts and overstock?\n",
    "4. What factors drive customer lifetime value?\n",
    "5. How effective are our marketing channels and campaigns?\n",
    "\n",
    "**Expected Deliverables**:\n",
    "- Customer segmentation model with personas\n",
    "- Product recommendation system\n",
    "- Inventory optimization strategy\n",
    "- Customer lifetime value analysis\n",
    "- Marketing attribution and ROI analysis\n",
    "\n",
    "**Skills Emphasized**:\n",
    "- Customer behavior analysis\n",
    "- Recommendation system development\n",
    "- Marketing analytics and attribution\n",
    "- Time series analysis for trends\n",
    "- A/B testing and experimentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requirements",
   "metadata": {},
   "source": [
    "## üìã Technical Requirements\n",
    "\n",
    "Your project must demonstrate mastery of all major Python concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "week1_requirements",
   "metadata": {},
   "source": [
    "### ‚úÖ Week 1 Fundamentals (25 points)\n",
    "\n",
    "**Data Types & Variables**\n",
    "- [ ] Effective use of strings, integers, floats, booleans\n",
    "- [ ] Proper type conversion and validation\n",
    "- [ ] Meaningful variable names following Python conventions\n",
    "\n",
    "**String Processing**\n",
    "- [ ] Advanced string manipulation for data cleaning\n",
    "- [ ] Professional formatting with f-strings\n",
    "- [ ] Text analysis and pattern matching\n",
    "\n",
    "**Lists & List Operations**\n",
    "- [ ] Complex list comprehensions for data transformation\n",
    "- [ ] List methods for data manipulation\n",
    "- [ ] Nested lists for structured data\n",
    "\n",
    "**Dictionaries & Data Structures**\n",
    "- [ ] Nested dictionaries for complex data modeling\n",
    "- [ ] Dictionary comprehensions for efficient processing\n",
    "- [ ] Using dictionaries as lookup tables and counters\n",
    "- [ ] Sets for data deduplication and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "week2_requirements",
   "metadata": {},
   "source": [
    "### ‚úÖ Week 2 Program Logic (25 points)\n",
    "\n",
    "**Conditional Statements**\n",
    "- [ ] Complex business logic with nested conditionals\n",
    "- [ ] Multiple condition evaluation with logical operators\n",
    "- [ ] Data validation and business rule enforcement\n",
    "\n",
    "**Loops & Iteration**\n",
    "- [ ] Efficient data processing with for loops\n",
    "- [ ] Conditional loops with while statements\n",
    "- [ ] Loop control with break and continue\n",
    "- [ ] Nested loops for complex analysis\n",
    "\n",
    "**Functions & Code Organization**\n",
    "- [ ] Well-designed functions with clear parameters and returns\n",
    "- [ ] Comprehensive docstrings and documentation\n",
    "- [ ] Advanced function techniques (*args, **kwargs)\n",
    "- [ ] Modular code design with separation of concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "week3_requirements",
   "metadata": {},
   "source": [
    "### ‚úÖ Week 3 Real-World Skills (30 points)\n",
    "\n",
    "**File I/O & Data Handling**\n",
    "- [ ] Robust CSV file processing with error handling\n",
    "- [ ] JSON data manipulation and export\n",
    "- [ ] Multiple file format support\n",
    "- [ ] Data backup and recovery procedures\n",
    "\n",
    "**Error Handling & Validation**\n",
    "- [ ] Comprehensive try-except-finally blocks\n",
    "- [ ] Graceful handling of missing or corrupted data\n",
    "- [ ] Input validation and sanitization\n",
    "- [ ] Meaningful error messages and logging\n",
    "\n",
    "**Data Analysis & Processing**\n",
    "- [ ] Complete data cleaning pipeline\n",
    "- [ ] Statistical analysis with Python fundamentals\n",
    "- [ ] Data transformation and enrichment\n",
    "- [ ] Insight generation and business intelligence\n",
    "\n",
    "**System Design & Architecture**\n",
    "- [ ] Object-oriented design with classes (bonus)\n",
    "- [ ] Modular system architecture\n",
    "- [ ] Scalable and maintainable code structure\n",
    "- [ ] Professional coding standards and practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deliverables",
   "metadata": {},
   "source": [
    "## üì¶ Project Deliverables\n",
    "\n",
    "Your final submission must include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical_deliverables",
   "metadata": {},
   "source": [
    "### üíª Technical Deliverables\n",
    "\n",
    "**1. Main Analysis Notebook (Required)**\n",
    "- Complete Jupyter notebook with all analysis code\n",
    "- Clear section headers and professional formatting\n",
    "- Code cells with comprehensive comments\n",
    "- Markdown cells explaining methodology and insights\n",
    "- All outputs displayed (no empty cells)\n",
    "\n",
    "**2. Data Processing Module (Required)**\n",
    "- Python script (.py file) with data processing functions\n",
    "- Modular design with reusable components\n",
    "- Comprehensive error handling throughout\n",
    "- Professional docstrings for all functions\n",
    "\n",
    "**3. Results Export (Required)**\n",
    "- Clean dataset exported to CSV format\n",
    "- Analysis results exported to JSON format\n",
    "- Summary statistics in structured format\n",
    "- Dashboard-ready data files\n",
    "\n",
    "**4. Supporting Files (Required)**\n",
    "- README.md with setup and usage instructions\n",
    "- requirements.txt or environment specification\n",
    "- Sample data files for testing\n",
    "- Configuration files if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "business_deliverables",
   "metadata": {},
   "source": [
    "### üìä Business Deliverables\n",
    "\n",
    "**1. Executive Summary (Required)**\n",
    "- 2-page business report in PDF or Markdown format\n",
    "- Clear problem statement and methodology\n",
    "- Key findings with supporting data\n",
    "- Actionable recommendations with impact estimates\n",
    "- Risk assessment and implementation considerations\n",
    "\n",
    "**2. Technical Documentation (Required)**\n",
    "- Data quality assessment report\n",
    "- Methodology explanation for non-technical audiences\n",
    "- Assumptions and limitations clearly stated\n",
    "- Validation approach and confidence levels\n",
    "\n",
    "**3. Presentation Materials (Required)**\n",
    "- 10-minute stakeholder presentation (slides or script)\n",
    "- Key insights visualized with charts/tables\n",
    "- Business impact quantified where possible\n",
    "- Next steps and recommendations clearly outlined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project_template",
   "metadata": {},
   "source": [
    "## üöÄ Project Implementation Template\n",
    "\n",
    "Use this structure to organize your capstone project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT SETUP AND IMPORTS\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter, defaultdict\n",
    "import random  # For generating sample data if needed\n",
    "\n",
    "# Project Configuration\n",
    "PROJECT_NAME = \"[Your Project Name]\"\n",
    "PROJECT_THEME = \"[Healthcare/Finance/Retail]\"\n",
    "AUTHOR = \"[Your Name]\"\n",
    "VERSION = \"1.0.0\"\n",
    "CREATED_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"üöÄ {PROJECT_NAME} - Capstone Project\")\n",
    "print(f\"üìä Theme: {PROJECT_THEME}\")\n",
    "print(f\"üë®‚Äçüíª Author: {AUTHOR}\")\n",
    "print(f\"üìÖ Created: {CREATED_DATE}\")\n",
    "print(f\"üîß Version: {VERSION}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading_section",
   "metadata": {},
   "source": [
    "### üì• Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING SYSTEM\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Professional data loading system with error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name):\n",
    "        self.project_name = project_name\n",
    "        self.data_sources = {}\n",
    "        self.load_errors = []\n",
    "        self.load_stats = {}\n",
    "    \n",
    "    def load_csv_data(self, filename, source_name):\n",
    "        \"\"\"\n",
    "        Load CSV data with comprehensive error handling\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Path to CSV file\n",
    "            source_name (str): Descriptive name for this data source\n",
    "        \n",
    "        Returns:\n",
    "            list: Loaded data records or empty list on failure\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"üìÇ Loading {source_name} from {filename}...\")\n",
    "            \n",
    "            # TODO: Implement robust CSV loading\n",
    "            # - Check if file exists\n",
    "            # - Handle different encodings\n",
    "            # - Validate CSV structure\n",
    "            # - Count records loaded\n",
    "            # - Log any issues found\n",
    "            \n",
    "            data = []\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row_num, row in enumerate(reader, 1):\n",
    "                    # Add row number for error tracking\n",
    "                    row['_row_number'] = row_num\n",
    "                    data.append(row)\n",
    "            \n",
    "            self.data_sources[source_name] = data\n",
    "            self.load_stats[source_name] = {\n",
    "                'records': len(data),\n",
    "                'columns': len(data[0].keys()) if data else 0,\n",
    "                'file_size': os.path.getsize(filename) if os.path.exists(filename) else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Loaded {len(data)} records from {source_name}\")\n",
    "            return data\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            error_msg = f\"File not found: {filename}\"\n",
    "            self.load_errors.append(error_msg)\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return []\n",
    "            \n",
    "        except csv.Error as e:\n",
    "            error_msg = f\"CSV parsing error in {filename}: {str(e)}\"\n",
    "            self.load_errors.append(error_msg)\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return []\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error loading {filename}: {str(e)}\"\n",
    "            self.load_errors.append(error_msg)\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_load_report(self):\n",
    "        \"\"\"Generate comprehensive data loading report\"\"\"\n",
    "        report = f\"\"\"\n",
    "        üìä DATA LOADING REPORT - {self.project_name}\n",
    "        {'='*50}\n",
    "        \n",
    "        üìà SUMMARY STATISTICS:\n",
    "        \"\"\"\n",
    "        \n",
    "        total_records = sum(stats['records'] for stats in self.load_stats.values())\n",
    "        total_sources = len(self.data_sources)\n",
    "        \n",
    "        report += f\"\"\"\n",
    "           Data Sources Loaded: {total_sources}\n",
    "           Total Records: {total_records:,}\n",
    "           Load Errors: {len(self.load_errors)}\n",
    "        \n",
    "        üìã SOURCE DETAILS:\n",
    "        \"\"\"\n",
    "        \n",
    "        for source, stats in self.load_stats.items():\n",
    "            report += f\"\"\"\n",
    "           {source}:\n",
    "             ‚Ä¢ Records: {stats['records']:,}\n",
    "             ‚Ä¢ Columns: {stats['columns']}\n",
    "             ‚Ä¢ File Size: {stats['file_size']:,} bytes\n",
    "            \"\"\"\n",
    "        \n",
    "        if self.load_errors:\n",
    "            report += \"\\n        ‚ö†Ô∏è ERRORS ENCOUNTERED:\\n\"\n",
    "            for i, error in enumerate(self.load_errors, 1):\n",
    "                report += f\"           {i}. {error}\\n\"\n",
    "        \n",
    "        return report.strip()\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(PROJECT_NAME)\n",
    "\n",
    "# TODO: Load your project data\n",
    "# Example usage:\n",
    "# primary_data = data_loader.load_csv_data('data/primary_dataset.csv', 'Primary Dataset')\n",
    "# secondary_data = data_loader.load_csv_data('data/secondary_dataset.csv', 'Secondary Dataset')\n",
    "\n",
    "print(\"\\nüìä Data loading system ready!\")\n",
    "print(\"üí° Implement your data loading in the cells below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_exploration",
   "metadata": {},
   "source": [
    "### üîç Data Exploration & Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA EXPLORATION AND QUALITY ASSESSMENT\n",
    "\n",
    "class DataExplorer:\n",
    "    \"\"\"Comprehensive data exploration and quality assessment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.exploration_results = {}\n",
    "        self.quality_issues = []\n",
    "    \n",
    "    def explore_dataset(self, data, dataset_name):\n",
    "        \"\"\"\n",
    "        Perform comprehensive data exploration\n",
    "        \n",
    "        Args:\n",
    "            data (list): Dataset to explore\n",
    "            dataset_name (str): Name of the dataset\n",
    "        \n",
    "        Returns:\n",
    "            dict: Exploration results\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            print(f\"‚ö†Ô∏è No data available for {dataset_name}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"\\nüîç Exploring {dataset_name}...\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        results = {\n",
    "            'dataset_name': dataset_name,\n",
    "            'record_count': len(data),\n",
    "            'column_count': len(data[0].keys()) if data else 0,\n",
    "            'columns': list(data[0].keys()) if data else [],\n",
    "            'data_types': {},\n",
    "            'missing_values': {},\n",
    "            'unique_values': {},\n",
    "            'sample_records': data[:3] if len(data) >= 3 else data\n",
    "        }\n",
    "        \n",
    "        # Analyze each column\n",
    "        for column in results['columns']:\n",
    "            values = [record.get(column, '') for record in data]\n",
    "            \n",
    "            # Count missing/empty values\n",
    "            missing_count = sum(1 for v in values if not v or str(v).strip() == '')\n",
    "            results['missing_values'][column] = missing_count\n",
    "            \n",
    "            # Count unique values\n",
    "            non_empty_values = [v for v in values if v and str(v).strip() != '']\n",
    "            results['unique_values'][column] = len(set(non_empty_values))\n",
    "            \n",
    "            # Determine data type\n",
    "            sample_values = [v for v in non_empty_values[:100] if v]  # Sample for type detection\n",
    "            if sample_values:\n",
    "                results['data_types'][column] = self._detect_data_type(sample_values)\n",
    "            else:\n",
    "                results['data_types'][column] = 'unknown'\n",
    "        \n",
    "        # Quality assessment\n",
    "        self._assess_data_quality(results, dataset_name)\n",
    "        \n",
    "        self.exploration_results[dataset_name] = results\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"üìä Dataset Overview:\")\n",
    "        print(f\"   Records: {results['record_count']:,}\")\n",
    "        print(f\"   Columns: {results['column_count']}\")\n",
    "        print(f\"   Columns: {', '.join(results['columns'])}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _detect_data_type(self, sample_values):\n",
    "        \"\"\"Detect the likely data type of a column\"\"\"\n",
    "        # TODO: Implement data type detection logic\n",
    "        # Check for:\n",
    "        # - Numeric (int/float)\n",
    "        # - Date/datetime\n",
    "        # - Boolean\n",
    "        # - Categorical\n",
    "        # - Text\n",
    "        \n",
    "        numeric_count = 0\n",
    "        date_count = 0\n",
    "        bool_count = 0\n",
    "        \n",
    "        for value in sample_values[:50]:  # Check first 50 values\n",
    "            str_val = str(value).strip().lower()\n",
    "            \n",
    "            # Check for boolean\n",
    "            if str_val in ['true', 'false', 'yes', 'no', '1', '0']:\n",
    "                bool_count += 1\n",
    "            \n",
    "            # Check for numeric\n",
    "            try:\n",
    "                float(str_val)\n",
    "                numeric_count += 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "            # Check for date patterns\n",
    "            if any(char in str_val for char in ['-', '/', ':']) and len(str_val) >= 8:\n",
    "                date_count += 1\n",
    "        \n",
    "        total_checked = len(sample_values[:50])\n",
    "        \n",
    "        if numeric_count / total_checked > 0.8:\n",
    "            return 'numeric'\n",
    "        elif date_count / total_checked > 0.6:\n",
    "            return 'date'\n",
    "        elif bool_count / total_checked > 0.8:\n",
    "            return 'boolean'\n",
    "        else:\n",
    "            return 'text'\n",
    "    \n",
    "    def _assess_data_quality(self, results, dataset_name):\n",
    "        \"\"\"Assess data quality and identify issues\"\"\"\n",
    "        total_records = results['record_count']\n",
    "        \n",
    "        for column, missing_count in results['missing_values'].items():\n",
    "            missing_rate = (missing_count / total_records) * 100\n",
    "            \n",
    "            if missing_rate > 50:\n",
    "                self.quality_issues.append(\n",
    "                    f\"{dataset_name}.{column}: {missing_rate:.1f}% missing values (critical)\"\n",
    "                )\n",
    "            elif missing_rate > 10:\n",
    "                self.quality_issues.append(\n",
    "                    f\"{dataset_name}.{column}: {missing_rate:.1f}% missing values (concerning)\"\n",
    "                )\n",
    "        \n",
    "        # Check for low cardinality in text fields\n",
    "        for column, unique_count in results['unique_values'].items():\n",
    "            if results['data_types'][column] == 'text' and unique_count == 1:\n",
    "                self.quality_issues.append(\n",
    "                    f\"{dataset_name}.{column}: Only one unique value (consider removing)\"\n",
    "                )\n",
    "    \n",
    "    def generate_exploration_report(self):\n",
    "        \"\"\"Generate comprehensive exploration report\"\"\"\n",
    "        if not self.exploration_results:\n",
    "            return \"No exploration results available.\"\n",
    "        \n",
    "        report = f\"\"\"\n",
    "üîç DATA EXPLORATION REPORT\n",
    "{'='*50}\n",
    "\n",
    "üìä DATASETS ANALYZED: {len(self.exploration_results)}\n",
    "\"\"\"\n",
    "        \n",
    "        for dataset_name, results in self.exploration_results.items():\n",
    "            report += f\"\"\"\n",
    "\n",
    "üìã {dataset_name.upper()}:\n",
    "   Records: {results['record_count']:,}\n",
    "   Columns: {results['column_count']}\n",
    "   \n",
    "   Column Details:\"\"\"\n",
    "            \n",
    "            for col in results['columns']:\n",
    "                missing_pct = (results['missing_values'][col] / results['record_count']) * 100\n",
    "                report += f\"\"\"\n",
    "   ‚Ä¢ {col}: {results['data_types'][col]} ({results['unique_values'][col]} unique, {missing_pct:.1f}% missing)\"\"\"\n",
    "        \n",
    "        if self.quality_issues:\n",
    "            report += \"\\n\\n‚ö†Ô∏è DATA QUALITY ISSUES:\\n\"\n",
    "            for i, issue in enumerate(self.quality_issues, 1):\n",
    "                report += f\"   {i}. {issue}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize data explorer\n",
    "data_explorer = DataExplorer()\n",
    "\n",
    "# TODO: Explore your loaded datasets\n",
    "# Example usage:\n",
    "# primary_exploration = data_explorer.explore_dataset(primary_data, 'Primary Dataset')\n",
    "# secondary_exploration = data_explorer.explore_dataset(secondary_data, 'Secondary Dataset')\n",
    "\n",
    "print(\"\\nüîç Data exploration system ready!\")\n",
    "print(\"üí° Use this to understand your data before cleaning and analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning_section",
   "metadata": {},
   "source": [
    "### üßπ Data Cleaning & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING AND VALIDATION SYSTEM\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"Professional data cleaning with validation and logging\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name):\n",
    "        self.project_name = project_name\n",
    "        self.cleaning_log = []\n",
    "        self.validation_rules = {}\n",
    "        self.cleaned_datasets = {}\n",
    "    \n",
    "    def add_validation_rule(self, column_name, rule_type, rule_value, error_message):\n",
    "        \"\"\"\n",
    "        Add a validation rule for a specific column\n",
    "        \n",
    "        Args:\n",
    "            column_name (str): Column to validate\n",
    "            rule_type (str): Type of rule ('required', 'numeric', 'range', 'format')\n",
    "            rule_value (any): Rule parameters\n",
    "            error_message (str): Message to log when rule fails\n",
    "        \"\"\"\n",
    "        if column_name not in self.validation_rules:\n",
    "            self.validation_rules[column_name] = []\n",
    "        \n",
    "        self.validation_rules[column_name].append({\n",
    "            'type': rule_type,\n",
    "            'value': rule_value,\n",
    "            'message': error_message\n",
    "        })\n",
    "    \n",
    "    def clean_dataset(self, data, dataset_name, custom_rules=None):\n",
    "        \"\"\"\n",
    "        Clean a dataset with comprehensive validation\n",
    "        \n",
    "        Args:\n",
    "            data (list): Raw dataset\n",
    "            dataset_name (str): Name of the dataset\n",
    "            custom_rules (dict): Custom cleaning rules\n",
    "        \n",
    "        Returns:\n",
    "            list: Cleaned dataset\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            self.cleaning_log.append(f\"No data to clean for {dataset_name}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nüßπ Cleaning {dataset_name}...\")\n",
    "        \n",
    "        cleaned_data = []\n",
    "        error_count = 0\n",
    "        \n",
    "        for i, record in enumerate(data):\n",
    "            try:\n",
    "                # Create cleaned record\n",
    "                cleaned_record = {}\n",
    "                record_valid = True\n",
    "                record_errors = []\n",
    "                \n",
    "                for column, value in record.items():\n",
    "                    if column.startswith('_'):  # Skip internal fields\n",
    "                        continue\n",
    "                    \n",
    "                    # Basic cleaning\n",
    "                    cleaned_value = self._clean_value(value)\n",
    "                    \n",
    "                    # Apply validation rules\n",
    "                    if column in self.validation_rules:\n",
    "                        is_valid, validation_errors = self._validate_value(\n",
    "                            cleaned_value, column, self.validation_rules[column]\n",
    "                        )\n",
    "                        \n",
    "                        if not is_valid:\n",
    "                            record_valid = False\n",
    "                            record_errors.extend(validation_errors)\n",
    "                    \n",
    "                    cleaned_record[column] = cleaned_value\n",
    "                \n",
    "                # Apply custom business logic\n",
    "                if custom_rules:\n",
    "                    cleaned_record, business_valid = self._apply_custom_rules(\n",
    "                        cleaned_record, custom_rules\n",
    "                    )\n",
    "                    record_valid = record_valid and business_valid\n",
    "                \n",
    "                # Add to cleaned data if valid\n",
    "                if record_valid:\n",
    "                    cleaned_data.append(cleaned_record)\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                    self.cleaning_log.append(\n",
    "                        f\"{dataset_name} row {i+1}: {'; '.join(record_errors)}\"\n",
    "                    )\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                self.cleaning_log.append(\n",
    "                    f\"{dataset_name} row {i+1}: Unexpected error - {str(e)}\"\n",
    "                )\n",
    "        \n",
    "        self.cleaned_datasets[dataset_name] = cleaned_data\n",
    "        \n",
    "        # Log cleaning results\n",
    "        success_rate = ((len(data) - error_count) / len(data)) * 100\n",
    "        print(f\"‚úÖ Cleaned {len(cleaned_data)}/{len(data)} records ({success_rate:.1f}% success rate)\")\n",
    "        \n",
    "        if error_count > 0:\n",
    "            print(f\"‚ö†Ô∏è {error_count} records had validation issues\")\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "    def _clean_value(self, value):\n",
    "        \"\"\"Apply basic cleaning to a value\"\"\"\n",
    "        if value is None:\n",
    "            return None\n",
    "        \n",
    "        # Convert to string and strip whitespace\n",
    "        str_value = str(value).strip()\n",
    "        \n",
    "        # Handle empty strings\n",
    "        if not str_value or str_value.lower() in ['', 'null', 'none', 'n/a', 'na']:\n",
    "            return None\n",
    "        \n",
    "        # TODO: Add more cleaning logic as needed:\n",
    "        # - Standardize case\n",
    "        # - Remove special characters\n",
    "        # - Format dates\n",
    "        # - Parse numbers\n",
    "        \n",
    "        return str_value\n",
    "    \n",
    "    def _validate_value(self, value, column_name, rules):\n",
    "        \"\"\"Validate a value against defined rules\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for rule in rules:\n",
    "            rule_type = rule['type']\n",
    "            rule_value = rule['value']\n",
    "            error_message = rule['message']\n",
    "            \n",
    "            if rule_type == 'required' and (value is None or value == ''):\n",
    "                errors.append(f\"{column_name}: {error_message}\")\n",
    "            \n",
    "            elif rule_type == 'numeric' and value is not None:\n",
    "                try:\n",
    "                    float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    errors.append(f\"{column_name}: {error_message}\")\n",
    "            \n",
    "            elif rule_type == 'range' and value is not None:\n",
    "                try:\n",
    "                    num_value = float(value)\n",
    "                    min_val, max_val = rule_value\n",
    "                    if not (min_val <= num_value <= max_val):\n",
    "                        errors.append(f\"{column_name}: {error_message}\")\n",
    "                except (ValueError, TypeError):\n",
    "                    pass  # Will be caught by numeric rule if present\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    def _apply_custom_rules(self, record, custom_rules):\n",
    "        \"\"\"Apply custom business logic rules\"\"\"\n",
    "        # TODO: Implement custom business rules\n",
    "        # This will vary by project theme\n",
    "        return record, True\n",
    "    \n",
    "    def export_cleaned_data(self, dataset_name, filename):\n",
    "        \"\"\"Export cleaned dataset to CSV\"\"\"\n",
    "        if dataset_name not in self.cleaned_datasets:\n",
    "            print(f\"‚ùå No cleaned data available for {dataset_name}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            data = self.cleaned_datasets[dataset_name]\n",
    "            \n",
    "            if not data:\n",
    "                print(f\"‚ö†Ô∏è No data to export for {dataset_name}\")\n",
    "                return False\n",
    "            \n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "                fieldnames = list(data[0].keys())\n",
    "                writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(data)\n",
    "            \n",
    "            print(f\"‚úÖ Exported {len(data)} cleaned records to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Export failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_cleaning_report(self):\n",
    "        \"\"\"Generate comprehensive cleaning report\"\"\"\n",
    "        report = f\"\"\"\n",
    "üßπ DATA CLEANING REPORT - {self.project_name}\n",
    "{'='*50}\n",
    "\n",
    "üìä CLEANING SUMMARY:\n",
    "   Datasets Processed: {len(self.cleaned_datasets)}\n",
    "   Validation Rules: {len(self.validation_rules)}\n",
    "   Issues Found: {len(self.cleaning_log)}\n",
    "\"\"\"\n",
    "        \n",
    "        for dataset_name, data in self.cleaned_datasets.items():\n",
    "            report += f\"\"\"\n",
    "   \n",
    "üìã {dataset_name.upper()}:\n",
    "   Clean Records: {len(data):,}\n",
    "\"\"\"\n",
    "        \n",
    "        if self.cleaning_log:\n",
    "            report += \"\\n\\n‚ö†Ô∏è CLEANING ISSUES:\\n\"\n",
    "            for i, issue in enumerate(self.cleaning_log[:10], 1):  # Show first 10\n",
    "                report += f\"   {i}. {issue}\\n\"\n",
    "            \n",
    "            if len(self.cleaning_log) > 10:\n",
    "                report += f\"   ... and {len(self.cleaning_log) - 10} more issues\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize data cleaner\n",
    "data_cleaner = DataCleaner(PROJECT_NAME)\n",
    "\n",
    "# TODO: Set up validation rules for your project\n",
    "# Example validation rules:\n",
    "# data_cleaner.add_validation_rule('age', 'required', None, 'Age is required')\n",
    "# data_cleaner.add_validation_rule('age', 'numeric', None, 'Age must be numeric')\n",
    "# data_cleaner.add_validation_rule('age', 'range', (0, 120), 'Age must be between 0 and 120')\n",
    "\n",
    "print(\"\\nüßπ Data cleaning system ready!\")\n",
    "print(\"üí° Define validation rules and clean your datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_section",
   "metadata": {},
   "source": [
    "### üìä Data Analysis & Business Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE ANALYSIS ENGINE\n",
    "\n",
    "class BusinessAnalyzer:\n",
    "    \"\"\"Advanced business analysis with Python fundamentals\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name, theme):\n",
    "        self.project_name = project_name\n",
    "        self.theme = theme\n",
    "        self.analysis_results = {}\n",
    "        self.insights = []\n",
    "        self.recommendations = []\n",
    "    \n",
    "    def calculate_descriptive_stats(self, data, numeric_columns, dataset_name):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive descriptive statistics\n",
    "        \n",
    "        Args:\n",
    "            data (list): Dataset to analyze\n",
    "            numeric_columns (list): List of numeric column names\n",
    "            dataset_name (str): Name of the dataset\n",
    "        \n",
    "        Returns:\n",
    "            dict: Descriptive statistics\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return {}\n",
    "        \n",
    "        print(f\"\\nüìä Calculating statistics for {dataset_name}...\")\n",
    "        \n",
    "        stats = {\n",
    "            'dataset_name': dataset_name,\n",
    "            'total_records': len(data),\n",
    "            'numeric_stats': {},\n",
    "            'categorical_stats': {}\n",
    "        }\n",
    "        \n",
    "        # Numeric statistics\n",
    "        for column in numeric_columns:\n",
    "            values = []\n",
    "            for record in data:\n",
    "                try:\n",
    "                    if record.get(column) is not None:\n",
    "                        values.append(float(record[column]))\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "            \n",
    "            if values:\n",
    "                sorted_values = sorted(values)\n",
    "                n = len(values)\n",
    "                \n",
    "                # Calculate comprehensive statistics\n",
    "                stats['numeric_stats'][column] = {\n",
    "                    'count': n,\n",
    "                    'sum': sum(values),\n",
    "                    'mean': sum(values) / n,\n",
    "                    'median': sorted_values[n//2] if n % 2 == 1 else (sorted_values[n//2-1] + sorted_values[n//2]) / 2,\n",
    "                    'min': min(values),\n",
    "                    'max': max(values),\n",
    "                    'range': max(values) - min(values),\n",
    "                    'q1': sorted_values[n//4],\n",
    "                    'q3': sorted_values[3*n//4],\n",
    "                    'std_dev': self._calculate_std_dev(values),\n",
    "                    'missing_count': len(data) - n\n",
    "                }\n",
    "        \n",
    "        # Categorical statistics\n",
    "        all_columns = set()\n",
    "        if data:\n",
    "            all_columns = set(data[0].keys())\n",
    "        \n",
    "        categorical_columns = all_columns - set(numeric_columns)\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            if column.startswith('_'):  # Skip internal fields\n",
    "                continue\n",
    "                \n",
    "            values = [record.get(column) for record in data if record.get(column)]\n",
    "            \n",
    "            if values:\n",
    "                value_counts = Counter(values)\n",
    "                stats['categorical_stats'][column] = {\n",
    "                    'unique_count': len(value_counts),\n",
    "                    'most_common': value_counts.most_common(5),\n",
    "                    'missing_count': len(data) - len(values)\n",
    "                }\n",
    "        \n",
    "        self.analysis_results[f'{dataset_name}_descriptive'] = stats\n",
    "        \n",
    "        print(f\"‚úÖ Statistics calculated for {len(numeric_columns)} numeric and {len(categorical_columns)} categorical columns\")\n",
    "        return stats\n",
    "    \n",
    "    def perform_segmentation_analysis(self, data, segment_column, value_columns, dataset_name):\n",
    "        \"\"\"\n",
    "        Perform customer/entity segmentation analysis\n",
    "        \n",
    "        Args:\n",
    "            data (list): Dataset to analyze\n",
    "            segment_column (str): Column to segment by\n",
    "            value_columns (list): Numeric columns to analyze per segment\n",
    "            dataset_name (str): Name of the dataset\n",
    "        \n",
    "        Returns:\n",
    "            dict: Segmentation analysis results\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return {}\n",
    "        \n",
    "        print(f\"\\nüéØ Performing segmentation analysis on {dataset_name}...\")\n",
    "        \n",
    "        segments = {}\n",
    "        \n",
    "        # Group data by segment\n",
    "        for record in data:\n",
    "            segment_value = record.get(segment_column)\n",
    "            if segment_value:\n",
    "                if segment_value not in segments:\n",
    "                    segments[segment_value] = []\n",
    "                segments[segment_value].append(record)\n",
    "        \n",
    "        # Analyze each segment\n",
    "        segment_analysis = {\n",
    "            'dataset_name': dataset_name,\n",
    "            'segment_column': segment_column,\n",
    "            'total_segments': len(segments),\n",
    "            'segment_details': {}\n",
    "        }\n",
    "        \n",
    "        for segment_name, segment_data in segments.items():\n",
    "            segment_stats = {\n",
    "                'count': len(segment_data),\n",
    "                'percentage': (len(segment_data) / len(data)) * 100,\n",
    "                'value_stats': {}\n",
    "            }\n",
    "            \n",
    "            # Calculate statistics for each value column\n",
    "            for column in value_columns:\n",
    "                values = []\n",
    "                for record in segment_data:\n",
    "                    try:\n",
    "                        if record.get(column) is not None:\n",
    "                            values.append(float(record[column]))\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                \n",
    "                if values:\n",
    "                    segment_stats['value_stats'][column] = {\n",
    "                        'count': len(values),\n",
    "                        'sum': sum(values),\n",
    "                        'mean': sum(values) / len(values),\n",
    "                        'min': min(values),\n",
    "                        'max': max(values)\n",
    "                    }\n",
    "            \n",
    "            segment_analysis['segment_details'][segment_name] = segment_stats\n",
    "        \n",
    "        self.analysis_results[f'{dataset_name}_segmentation'] = segment_analysis\n",
    "        \n",
    "        print(f\"‚úÖ Segmentation analysis complete: {len(segments)} segments identified\")\n",
    "        return segment_analysis\n",
    "    \n",
    "    def perform_trend_analysis(self, data, date_column, value_column, dataset_name):\n",
    "        \"\"\"\n",
    "        Perform time-based trend analysis\n",
    "        \n",
    "        Args:\n",
    "            data (list): Dataset to analyze\n",
    "            date_column (str): Column containing date information\n",
    "            value_column (str): Column to analyze trends for\n",
    "            dataset_name (str): Name of the dataset\n",
    "        \n",
    "        Returns:\n",
    "            dict: Trend analysis results\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return {}\n",
    "        \n",
    "        print(f\"\\nüìà Performing trend analysis on {dataset_name}...\")\n",
    "        \n",
    "        # Group data by time periods\n",
    "        time_groups = {}\n",
    "        \n",
    "        for record in data:\n",
    "            date_str = record.get(date_column)\n",
    "            value_str = record.get(value_column)\n",
    "            \n",
    "            if date_str and value_str:\n",
    "                try:\n",
    "                    # Extract year-month for grouping\n",
    "                    time_key = date_str[:7]  # YYYY-MM format\n",
    "                    value = float(value_str)\n",
    "                    \n",
    "                    if time_key not in time_groups:\n",
    "                        time_groups[time_key] = []\n",
    "                    time_groups[time_key].append(value)\n",
    "                    \n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "        \n",
    "        # Calculate trend statistics\n",
    "        trend_data = []\n",
    "        for time_period in sorted(time_groups.keys()):\n",
    "            values = time_groups[time_period]\n",
    "            trend_data.append({\n",
    "                'period': time_period,\n",
    "                'count': len(values),\n",
    "                'sum': sum(values),\n",
    "                'mean': sum(values) / len(values),\n",
    "                'min': min(values),\n",
    "                'max': max(values)\n",
    "            })\n",
    "        \n",
    "        # Calculate growth rates\n",
    "        for i in range(1, len(trend_data)):\n",
    "            prev_value = trend_data[i-1]['sum']\n",
    "            curr_value = trend_data[i]['sum']\n",
    "            \n",
    "            if prev_value != 0:\n",
    "                growth_rate = ((curr_value - prev_value) / prev_value) * 100\n",
    "                trend_data[i]['growth_rate'] = growth_rate\n",
    "            else:\n",
    "                trend_data[i]['growth_rate'] = 0\n",
    "        \n",
    "        trend_analysis = {\n",
    "            'dataset_name': dataset_name,\n",
    "            'date_column': date_column,\n",
    "            'value_column': value_column,\n",
    "            'periods_analyzed': len(trend_data),\n",
    "            'trend_data': trend_data\n",
    "        }\n",
    "        \n",
    "        self.analysis_results[f'{dataset_name}_trends'] = trend_analysis\n",
    "        \n",
    "        print(f\"‚úÖ Trend analysis complete: {len(trend_data)} time periods analyzed\")\n",
    "        return trend_analysis\n",
    "    \n",
    "    def _calculate_std_dev(self, values):\n",
    "        \"\"\"Calculate standard deviation\"\"\"\n",
    "        if len(values) < 2:\n",
    "            return 0\n",
    "        \n",
    "        mean = sum(values) / len(values)\n",
    "        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)\n",
    "        return variance ** 0.5\n",
    "    \n",
    "    def generate_insights(self):\n",
    "        \"\"\"\n",
    "        Generate business insights from analysis results\n",
    "        \"\"\"\n",
    "        print(f\"\\nüí° Generating business insights...\")\n",
    "        \n",
    "        self.insights = []\n",
    "        \n",
    "        # TODO: Implement insight generation based on your theme\n",
    "        # This is where you interpret the analysis results\n",
    "        # and generate actionable business insights\n",
    "        \n",
    "        # Example insights (customize for your theme):\n",
    "        for analysis_name, results in self.analysis_results.items():\n",
    "            if 'descriptive' in analysis_name:\n",
    "                self._generate_descriptive_insights(results)\n",
    "            elif 'segmentation' in analysis_name:\n",
    "                self._generate_segmentation_insights(results)\n",
    "            elif 'trends' in analysis_name:\n",
    "                self._generate_trend_insights(results)\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(self.insights)} business insights\")\n",
    "        return self.insights\n",
    "    \n",
    "    def _generate_descriptive_insights(self, stats):\n",
    "        \"\"\"Generate insights from descriptive statistics\"\"\"\n",
    "        # TODO: Customize based on your project theme\n",
    "        pass\n",
    "    \n",
    "    def _generate_segmentation_insights(self, analysis):\n",
    "        \"\"\"Generate insights from segmentation analysis\"\"\"\n",
    "        # TODO: Customize based on your project theme\n",
    "        pass\n",
    "    \n",
    "    def _generate_trend_insights(self, analysis):\n",
    "        \"\"\"Generate insights from trend analysis\"\"\"\n",
    "        # TODO: Customize based on your project theme\n",
    "        pass\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"\n",
    "        Generate actionable business recommendations\n",
    "        \"\"\"\n",
    "        print(f\"\\nüéØ Generating business recommendations...\")\n",
    "        \n",
    "        self.recommendations = []\n",
    "        \n",
    "        # TODO: Implement recommendation generation\n",
    "        # Based on your insights, what actions should the business take?\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(self.recommendations)} recommendations\")\n",
    "        return self.recommendations\n",
    "\n",
    "# Initialize business analyzer\n",
    "business_analyzer = BusinessAnalyzer(PROJECT_NAME, PROJECT_THEME)\n",
    "\n",
    "print(f\"\\nüìä Business analysis system ready for {PROJECT_THEME} theme!\")\n",
    "print(\"üí° Use this to perform comprehensive data analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_export_section",
   "metadata": {},
   "source": [
    "### üì§ Results Export & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE EXPORT AND REPORTING SYSTEM\n",
    "\n",
    "class ResultsExporter:\n",
    "    \"\"\"Professional results export and reporting system\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name, author):\n",
    "        self.project_name = project_name\n",
    "        self.author = author\n",
    "        self.export_log = []\n",
    "    \n",
    "    def export_analysis_results(self, analysis_results, filename='analysis_results.json'):\n",
    "        \"\"\"\n",
    "        Export analysis results to JSON format\n",
    "        \n",
    "        Args:\n",
    "            analysis_results (dict): Analysis results to export\n",
    "            filename (str): Output filename\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare export data with metadata\n",
    "            export_data = {\n",
    "                'metadata': {\n",
    "                    'project_name': self.project_name,\n",
    "                    'author': self.author,\n",
    "                    'export_timestamp': datetime.now().isoformat(),\n",
    "                    'version': '1.0.0'\n",
    "                },\n",
    "                'analysis_results': analysis_results\n",
    "            }\n",
    "            \n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(export_data, file, indent=2, default=str)\n",
    "            \n",
    "            self.export_log.append(f\"Analysis results exported to {filename}\")\n",
    "            print(f\"‚úÖ Analysis results exported to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to export analysis results: {str(e)}\"\n",
    "            self.export_log.append(error_msg)\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return False\n",
    "    \n",
    "    def export_dashboard_data(self, cleaned_datasets, analysis_results, filename='dashboard_data.json'):\n",
    "        \"\"\"\n",
    "        Export dashboard-ready data\n",
    "        \n",
    "        Args:\n",
    "            cleaned_datasets (dict): Cleaned datasets\n",
    "            analysis_results (dict): Analysis results\n",
    "            filename (str): Output filename\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare dashboard data\n",
    "            dashboard_data = {\n",
    "                'metadata': {\n",
    "                    'project_name': self.project_name,\n",
    "                    'created_by': self.author,\n",
    "                    'created_at': datetime.now().isoformat(),\n",
    "                    'data_sources': list(cleaned_datasets.keys())\n",
    "                },\n",
    "                'summary_metrics': self._generate_summary_metrics(cleaned_datasets, analysis_results),\n",
    "                'visualizations': self._prepare_visualization_data(analysis_results),\n",
    "                'datasets': {name: data[:100] for name, data in cleaned_datasets.items()}  # Sample data\n",
    "            }\n",
    "            \n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(dashboard_data, file, indent=2, default=str)\n",
    "            \n",
    "            self.export_log.append(f\"Dashboard data exported to {filename}\")\n",
    "            print(f\"‚úÖ Dashboard data exported to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to export dashboard data: {str(e)}\"\n",
    "            self.export_log.append(error_msg)\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_executive_summary(self, insights, recommendations, filename='executive_summary.md'):\n",
    "        \"\"\"\n",
    "        Generate executive summary report\n",
    "        \n",
    "        Args:\n",
    "            insights (list): Business insights\n",
    "            recommendations (list): Business recommendations\n",
    "            filename (str): Output filename\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            summary = f\"\"\"\n",
    "# Executive Summary: {self.project_name}\n",
    "\n",
    "**Author:** {self.author}  \n",
    "**Date:** {datetime.now().strftime('%B %d, %Y')}  \n",
    "**Project Type:** Capstone Data Analysis Project  \n",
    "\n",
    "## Executive Overview\n",
    "\n",
    "This analysis provides data-driven insights and recommendations for [BUSINESS CONTEXT]. Using comprehensive data analysis techniques, we examined [DATA DESCRIPTION] to identify opportunities for improvement and strategic decision-making.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "            # Add insights\n",
    "            if insights:\n",
    "                for i, insight in enumerate(insights, 1):\n",
    "                    summary += f\"\\n{i}. {insight}\\n\"\n",
    "            else:\n",
    "                summary += \"\\n*Key insights to be added based on analysis results.*\\n\"\n",
    "            \n",
    "            summary += \"\\n## Recommendations\\n\"\n",
    "            \n",
    "            # Add recommendations\n",
    "            if recommendations:\n",
    "                for i, recommendation in enumerate(recommendations, 1):\n",
    "                    summary += f\"\\n{i}. {recommendation}\\n\"\n",
    "            else:\n",
    "                summary += \"\\n*Actionable recommendations to be added based on insights.*\\n\"\n",
    "            \n",
    "            summary += f\"\"\"\n",
    "\n",
    "## Implementation Impact\n",
    "\n",
    "The recommendations outlined in this analysis are expected to deliver measurable business value through:\n",
    "\n",
    "- **Operational Efficiency:** [Quantify expected improvements]\n",
    "- **Cost Reduction:** [Estimate potential savings]\n",
    "- **Revenue Growth:** [Project revenue impact]\n",
    "- **Risk Mitigation:** [Identify risks addressed]\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Immediate Actions** (0-30 days): [List quick wins]\n",
    "2. **Short-term Initiatives** (1-3 months): [List medium-term projects]\n",
    "3. **Long-term Strategy** (3-12 months): [List strategic initiatives]\n",
    "\n",
    "## Methodology\n",
    "\n",
    "This analysis was conducted using Python data analysis techniques including:\n",
    "- Comprehensive data cleaning and validation\n",
    "- Descriptive and inferential statistical analysis\n",
    "- Segmentation and trend analysis\n",
    "- Business intelligence generation\n",
    "\n",
    "## Appendix\n",
    "\n",
    "- **Data Sources:** [List primary data sources]\n",
    "- **Analysis Period:** [Specify time range]\n",
    "- **Limitations:** [Note any analysis limitations]\n",
    "- **Confidence Level:** [State confidence in findings]\n",
    "\n",
    "---\n",
    "\n",
    "*This report was generated as part of the Python Fundamentals for Data Science capstone project.*\n",
    "\"\"\"\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(summary.strip())\n",
    "            \n",
    "            self.export_log.append(f\"Executive summary generated: {filename}\")\n",
    "            print(f\"‚úÖ Executive summary generated: {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to generate executive summary: {str(e)}\"\n",
    "            self.export_log.append(error_msg)\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_technical_documentation(self, data_sources, cleaning_log, analysis_methods, filename='technical_documentation.md'):\n",
    "        \"\"\"\n",
    "        Generate technical documentation\n",
    "        \n",
    "        Args:\n",
    "            data_sources (dict): Information about data sources\n",
    "            cleaning_log (list): Data cleaning log\n",
    "            analysis_methods (list): Analysis methods used\n",
    "            filename (str): Output filename\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            documentation = f\"\"\"\n",
    "# Technical Documentation: {self.project_name}\n",
    "\n",
    "**Author:** {self.author}  \n",
    "**Date:** {datetime.now().strftime('%B %d, %Y')}  \n",
    "**Version:** 1.0.0  \n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This document provides technical details about the data analysis methodology, data quality assessment, and implementation approach for the {self.project_name} project.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "            # Add data source information\n",
    "            if data_sources:\n",
    "                for source_name, source_info in data_sources.items():\n",
    "                    documentation += f\"\"\"\n",
    "### {source_name}\n",
    "- **Records:** {source_info.get('records', 'Unknown'):,}\n",
    "- **Columns:** {source_info.get('columns', 'Unknown')}\n",
    "- **File Size:** {source_info.get('file_size', 0):,} bytes\n",
    "\"\"\"\n",
    "            \n",
    "            documentation += \"\\n## Data Quality Assessment\\n\"\n",
    "            \n",
    "            # Add data quality information\n",
    "            if cleaning_log:\n",
    "                documentation += f\"\\n**Issues Identified:** {len(cleaning_log)}\\n\\n\"\n",
    "                documentation += \"**Sample Issues:**\\n\"\n",
    "                for i, issue in enumerate(cleaning_log[:5], 1):\n",
    "                    documentation += f\"{i}. {issue}\\n\"\n",
    "                \n",
    "                if len(cleaning_log) > 5:\n",
    "                    documentation += f\"\\n*... and {len(cleaning_log) - 5} additional issues addressed.*\\n\"\n",
    "            else:\n",
    "                documentation += \"\\nNo significant data quality issues identified.\\n\"\n",
    "            \n",
    "            documentation += f\"\"\"\n",
    "\n",
    "## Analysis Methodology\n",
    "\n",
    "### Data Processing Pipeline\n",
    "\n",
    "1. **Data Loading:** Robust CSV parsing with encoding detection\n",
    "2. **Data Exploration:** Comprehensive profiling and quality assessment\n",
    "3. **Data Cleaning:** Validation rules and business logic application\n",
    "4. **Statistical Analysis:** Descriptive statistics and segmentation\n",
    "5. **Insight Generation:** Business intelligence and recommendations\n",
    "\n",
    "### Python Implementation\n",
    "\n",
    "The analysis was implemented using core Python functionality:\n",
    "\n",
    "- **Data Structures:** Lists and dictionaries for data manipulation\n",
    "- **Control Flow:** Loops and conditionals for processing logic\n",
    "- **Functions:** Modular design for reusability and maintainability\n",
    "- **File I/O:** CSV and JSON handling with error management\n",
    "- **Error Handling:** Comprehensive try-except blocks for robustness\n",
    "\n",
    "### Statistical Methods\n",
    "\n",
    "- Descriptive statistics (mean, median, standard deviation)\n",
    "- Segmentation analysis with comparative metrics\n",
    "- Trend analysis with growth rate calculations\n",
    "- Distribution analysis and outlier detection\n",
    "\n",
    "## Validation and Testing\n",
    "\n",
    "### Data Validation Rules\n",
    "- Required field validation\n",
    "- Numeric range checking\n",
    "- Format consistency verification\n",
    "- Business rule compliance\n",
    "\n",
    "### Quality Assurance\n",
    "- Unit testing of analysis functions\n",
    "- Cross-validation of statistical calculations\n",
    "- Sample verification of results\n",
    "- Edge case handling verification\n",
    "\n",
    "## Limitations and Assumptions\n",
    "\n",
    "### Data Limitations\n",
    "- [Document any data coverage gaps]\n",
    "- [Note temporal limitations]\n",
    "- [Identify missing variables]\n",
    "\n",
    "### Analytical Assumptions\n",
    "- [List key assumptions made]\n",
    "- [Note statistical assumptions]\n",
    "- [Document business logic assumptions]\n",
    "\n",
    "## Future Enhancements\n",
    "\n",
    "### Immediate Improvements\n",
    "- Advanced statistical modeling\n",
    "- Machine learning integration\n",
    "- Real-time data processing\n",
    "\n",
    "### Scalability Considerations\n",
    "- Database integration\n",
    "- Cloud deployment\n",
    "- Performance optimization\n",
    "\n",
    "---\n",
    "\n",
    "*This documentation was generated as part of the Python Fundamentals for Data Science capstone project.*\n",
    "\"\"\"\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(documentation.strip())\n",
    "            \n",
    "            self.export_log.append(f\"Technical documentation generated: {filename}\")\n",
    "            print(f\"‚úÖ Technical documentation generated: {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to generate technical documentation: {str(e)}\"\n",
    "            self.export_log.append(error_msg)\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return False\n",
    "    \n",
    "    def _generate_summary_metrics(self, datasets, analysis_results):\n",
    "        \"\"\"Generate high-level summary metrics\"\"\"\n",
    "        summary = {\n",
    "            'total_records': sum(len(data) for data in datasets.values()),\n",
    "            'datasets_processed': len(datasets),\n",
    "            'analyses_performed': len(analysis_results)\n",
    "        }\n",
    "        \n",
    "        # TODO: Add theme-specific summary metrics\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _prepare_visualization_data(self, analysis_results):\n",
    "        \"\"\"Prepare data for visualization tools\"\"\"\n",
    "        viz_data = {\n",
    "            'charts': [],\n",
    "            'tables': [],\n",
    "            'metrics': []\n",
    "        }\n",
    "        \n",
    "        # TODO: Extract key data for charts and visualizations\n",
    "        \n",
    "        return viz_data\n",
    "    \n",
    "    def generate_export_summary(self):\n",
    "        \"\"\"Generate summary of all exports\"\"\"\n",
    "        summary = f\"\"\"\n",
    "üì§ EXPORT SUMMARY - {self.project_name}\n",
    "{'='*50}\n",
    "\n",
    "üìä Export Operations: {len(self.export_log)}\n",
    "üìÖ Generated: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\n",
    "üë®‚Äçüíª Author: {self.author}\n",
    "\n",
    "üìã Files Generated:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, log_entry in enumerate(self.export_log, 1):\n",
    "            summary += f\"   {i}. {log_entry}\\n\"\n",
    "        \n",
    "        return summary.strip()\n",
    "\n",
    "# Initialize results exporter\n",
    "results_exporter = ResultsExporter(PROJECT_NAME, AUTHOR)\n",
    "\n",
    "print(\"\\nüì§ Results export system ready!\")\n",
    "print(\"üí° Use this to generate professional reports and documentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project_execution",
   "metadata": {},
   "source": [
    "## üöÄ Project Execution Section\n",
    "\n",
    "**This is where you implement your complete capstone project!**\n",
    "\n",
    "Follow these steps to build your data science system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: PROJECT SETUP AND DATA PREPARATION\n",
    "\n",
    "print(\"üöÄ STEP 1: PROJECT SETUP AND DATA PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: 1. Choose your project theme and update constants\n",
    "# Update these variables at the top of the notebook:\n",
    "# PROJECT_NAME = \"Your Project Name\"\n",
    "# PROJECT_THEME = \"Healthcare/Finance/Retail\"\n",
    "# AUTHOR = \"Your Name\"\n",
    "\n",
    "# TODO: 2. Create or load your dataset\n",
    "# Option A: Create sample data for your theme\n",
    "# Option B: Load provided datasets\n",
    "# Option C: Use your own data (with instructor approval)\n",
    "\n",
    "# Example data loading:\n",
    "# primary_data = data_loader.load_csv_data('data/primary_dataset.csv', 'Primary Dataset')\n",
    "# secondary_data = data_loader.load_csv_data('data/secondary_dataset.csv', 'Secondary Dataset')\n",
    "\n",
    "print(\"üìã TODO List for Step 1:\")\nprint(\"   [ ] Choose project theme\")\nprint(\"   [ ] Update project constants\")\nprint(\"   [ ] Load or create datasets\")\nprint(\"   [ ] Verify data loading success\")\nprint(\"   [ ] Generate data loading report\")\n\nprint(\"\\nüí° Once Step 1 is complete, move to Step 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2_exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: DATA EXPLORATION AND QUALITY ASSESSMENT\n",
    "\n",
    "print(\"üîç STEP 2: DATA EXPLORATION AND QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: 3. Explore your datasets\n",
    "# primary_exploration = data_explorer.explore_dataset(primary_data, 'Primary Dataset')\n",
    "# secondary_exploration = data_explorer.explore_dataset(secondary_data, 'Secondary Dataset')\n",
    "\n",
    "# TODO: 4. Generate exploration report\n",
    "# exploration_report = data_explorer.generate_exploration_report()\n",
    "# print(exploration_report)\n",
    "\n",
    "print(\"üìã TODO List for Step 2:\")\nprint(\"   [ ] Explore each dataset\")\nprint(\"   [ ] Identify data types and patterns\")\nprint(\"   [ ] Assess data quality issues\")\nprint(\"   [ ] Document findings\")\nprint(\"   [ ] Plan cleaning strategy\")\n\nprint(\"\\nüí° Use exploration results to plan your data cleaning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3_cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: DATA CLEANING AND VALIDATION\n",
    "\n",
    "print(\"üßπ STEP 3: DATA CLEANING AND VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: 5. Define validation rules for your data\n",
    "# Example validation rules (customize for your theme):\n",
    "# data_cleaner.add_validation_rule('age', 'required', None, 'Age is required')\n",
    "# data_cleaner.add_validation_rule('age', 'numeric', None, 'Age must be numeric')\n",
    "# data_cleaner.add_validation_rule('age', 'range', (0, 120), 'Age must be between 0 and 120')\n",
    "\n",
    "# TODO: 6. Clean your datasets\n",
    "# clean_primary = data_cleaner.clean_dataset(primary_data, 'Primary Dataset')\n",
    "# clean_secondary = data_cleaner.clean_dataset(secondary_data, 'Secondary Dataset')\n",
    "\n",
    "# TODO: 7. Export cleaned data\n",
    "# data_cleaner.export_cleaned_data('Primary Dataset', 'clean_primary_data.csv')\n",
    "# data_cleaner.export_cleaned_data('Secondary Dataset', 'clean_secondary_data.csv')\n",
    "\n",
    "# TODO: 8. Generate cleaning report\n",
    "# cleaning_report = data_cleaner.generate_cleaning_report()\n",
    "# print(cleaning_report)\n",
    "\n",
    "print(\"üìã TODO List for Step 3:\")\nprint(\"   [ ] Define validation rules\")\nprint(\"   [ ] Clean all datasets\")\nprint(\"   [ ] Export cleaned data\")\nprint(\"   [ ] Generate cleaning report\")\nprint(\"   [ ] Verify data quality\")\n\nprint(\"\\nüí° Clean data is the foundation of good analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: COMPREHENSIVE DATA ANALYSIS\n",
    "\n",
    "print(\"üìä STEP 4: COMPREHENSIVE DATA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: 9. Perform descriptive analysis\n",
    "# Define which columns are numeric for your analysis\n",
    "# numeric_columns = ['age', 'income', 'score']  # Customize for your data\n",
    "# descriptive_stats = business_analyzer.calculate_descriptive_stats(\n",
    "#     clean_primary, numeric_columns, 'Primary Dataset'\n",
    "# )\n",
    "\n",
    "# TODO: 10. Perform segmentation analysis\n",
    "# segment_analysis = business_analyzer.perform_segmentation_analysis(\n",
    "#     clean_primary, 'category', numeric_columns, 'Primary Dataset'\n",
    "# )\n",
    "\n",
    "# TODO: 11. Perform trend analysis (if applicable)\n",
    "# trend_analysis = business_analyzer.perform_trend_analysis(\n",
    "#     clean_primary, 'date', 'value', 'Primary Dataset'\n",
    "# )\n",
    "\n",
    "print(\"üìã TODO List for Step 4:\")\nprint(\"   [ ] Define numeric columns for analysis\")\nprint(\"   [ ] Calculate descriptive statistics\")\nprint(\"   [ ] Perform segmentation analysis\")\nprint(\"   [ ] Conduct trend analysis\")\nprint(\"   [ ] Identify key patterns\")\n\nprint(\"\\nüí° This is where you discover the insights in your data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5_insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: INSIGHT GENERATION AND RECOMMENDATIONS\n",
    "\n",
    "print(\"üí° STEP 5: INSIGHT GENERATION AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: 12. Generate business insights\n",
    "# insights = business_analyzer.generate_insights()\n",
    "# print(\"\\nüîç Key Business Insights:\")\n",
    "# for i, insight in enumerate(insights, 1):\n",
    "#     print(f\"   {i}. {insight}\")\n",
    "\n",
    "# TODO: 13. Generate recommendations\n",
    "# recommendations = business_analyzer.generate_recommendations()\n",
    "# print(\"\\nüéØ Business Recommendations:\")\n",
    "# for i, recommendation in enumerate(recommendations, 1):\n",
    "#     print(f\"   {i}. {recommendation}\")\n",
    "\n",
    "print(\"üìã TODO List for Step 5:\")\nprint(\"   [ ] Analyze statistical results\")\nprint(\"   [ ] Generate business insights\")\nprint(\"   [ ] Create actionable recommendations\")\nprint(\"   [ ] Quantify potential impact\")\nprint(\"   [ ] Validate insights with data\")\n\nprint(\"\\nüí° Transform your analysis into business value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6_export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: PROFESSIONAL REPORTING AND EXPORT\n",
    "\n",
    "print(\"üì§ STEP 6: PROFESSIONAL REPORTING AND EXPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: 14. Export analysis results\n",
    "# results_exporter.export_analysis_results(\n",
    "#     business_analyzer.analysis_results, \n",
    "#     'capstone_analysis_results.json'\n",
    "# )\n",
    "\n",
    "# TODO: 15. Export dashboard data\n",
    "# results_exporter.export_dashboard_data(\n",
    "#     data_cleaner.cleaned_datasets,\n",
    "#     business_analyzer.analysis_results,\n",
    "#     'capstone_dashboard_data.json'\n",
    "# )\n",
    "\n",
    "# TODO: 16. Generate executive summary\n",
    "# results_exporter.generate_executive_summary(\n",
    "#     insights, recommendations, 'capstone_executive_summary.md'\n",
    "# )\n",
    "\n",
    "# TODO: 17. Generate technical documentation\n",
    "# results_exporter.generate_technical_documentation(\n",
    "#     data_loader.load_stats,\n",
    "#     data_cleaner.cleaning_log,\n",
    "#     ['Descriptive Statistics', 'Segmentation Analysis', 'Trend Analysis'],\n",
    "#     'capstone_technical_docs.md'\n",
    "# )\n",
    "\n",
    "# TODO: 18. Generate export summary\n",
    "# export_summary = results_exporter.generate_export_summary()\n",
    "# print(export_summary)\n",
    "\n",
    "print(\"üìã TODO List for Step 6:\")\nprint(\"   [ ] Export analysis results to JSON\")\nprint(\"   [ ] Create dashboard data export\")\nprint(\"   [ ] Generate executive summary\")\nprint(\"   [ ] Create technical documentation\")\nprint(\"   [ ] Verify all exports successful\")\n\nprint(\"\\nüí° Professional reporting makes your work presentation-ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission_section",
   "metadata": {},
   "source": [
    "## üìã Project Submission Checklist\n",
    "\n",
    "**Before submitting your capstone project, verify you have completed all requirements:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "submission_checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL PROJECT SUBMISSION CHECKLIST\n",
    "\n",
    "print(\"üìã CAPSTONE PROJECT SUBMISSION CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "submission_checklist = {\n",
    "    \"Technical Requirements (25 points)\": {\n",
    "        \"Week 1 Fundamentals\": [\n",
    "            \"Effective use of all basic data types\",\n",
    "            \"Advanced string processing for data cleaning\",\n",
    "            \"Complex list operations and comprehensions\",\n",
    "            \"Sophisticated dictionary usage for data modeling\"\n",
    "        ],\n",
    "        \"Week 2 Program Logic\": [\n",
    "            \"Complex conditional statements for business logic\",\n",
    "            \"Efficient loops for data processing\",\n",
    "            \"Well-designed functions with proper documentation\",\n",
    "            \"Modular code organization\"\n",
    "        ],\n",
    "        \"Week 3 Real-World Skills\": [\n",
    "            \"Robust file I/O with error handling\",\n",
    "            \"Comprehensive data cleaning pipeline\",\n",
    "            \"Statistical analysis with Python fundamentals\",\n",
    "            \"Professional system architecture\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Analysis Quality (25 points)\": [\n",
    "        \"Comprehensive data exploration and profiling\",\n",
    "        \"Thorough data cleaning with validation\",\n",
    "        \"Multiple analysis techniques applied\",\n",
    "        \"Statistical results are mathematically correct\",\n",
    "        \"Insights are backed by solid evidence\",\n",
    "        \"Edge cases and limitations addressed\"\n",
    "    ],\n",
    "    \n",
    "    \"Business Impact (25 points)\": [\n",
    "        \"Clear problem statement and business context\",\n",
    "        \"Actionable insights that drive decisions\",\n",
    "        \"Specific recommendations with impact estimates\",\n",
    "        \"Risk assessment and implementation guidance\",\n",
    "        \"Results communicated for business audience\",\n",
    "        \"Portfolio-ready professional quality\"\n",
    "    ],\n",
    "    \n",
    "    \"Documentation & Presentation (15 points)\": [\n",
    "        \"Comprehensive code documentation\",\n",
    "        \"Executive summary report completed\",\n",
    "        \"Technical documentation provided\",\n",
    "        \"Results exported in multiple formats\",\n",
    "        \"Presentation materials prepared\"\n",
    "    ],\n",
    "    \n",
    "    \"Code Quality (10 points)\": [\n",
    "        \"Clean, readable, and well-organized code\",\n",
    "        \"Meaningful variable and function names\",\n",
    "        \"Proper error handling throughout\",\n",
    "        \"Professional coding standards followed\",\n",
    "        \"System runs without errors\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, requirements in submission_checklist.items():\n",
    "    print(f\"\\nüéØ {category}:\")\n",
    "    if isinstance(requirements, dict):\n",
    "        for subcategory, items in requirements.items():\n",
    "            print(f\"\\n   üìã {subcategory}:\")\n",
    "            for item in items:\n",
    "                print(f\"      [ ] {item}\")\n",
    "    else:\n",
    "        for item in requirements:\n",
    "            print(f\"   [ ] {item}\")\n",
    "\n",
    "print(\"\\n\\nüì§ SUBMISSION DELIVERABLES:\")\ndeliverables = [\n",
    "    \"Complete Jupyter notebook with all analysis code\",\n",
    "    \"Cleaned datasets exported to CSV format\",\n",
    "    \"Analysis results exported to JSON format\",\n",
    "    \"Executive summary report (PDF or Markdown)\",\n",
    "    \"Technical documentation (Markdown)\",\n",
    "    \"README.md with setup and usage instructions\",\n",
    "    \"Presentation slides or script (10 minutes)\"\n",
    "]\n",
    "\n",
    "for i, deliverable in enumerate(deliverables, 1):\n",
    "    print(f\"   {i}. [ ] {deliverable}\")\n",
    "\n",
    "print(\"\\n\\nüéØ FINAL VERIFICATION:\")\nverification_steps = [\n",
    "    \"All code cells run without errors\",\n",
    "    \"Analysis produces meaningful insights\",\n",
    "    \"Business recommendations are actionable\",\n",
    "    \"Documentation is complete and professional\",\n",
    "    \"Project demonstrates mastery of Python fundamentals\",\n",
    "    \"Work is suitable for job application portfolio\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(verification_steps, 1):\n",
    "    print(f\"   {i}. [ ] {step}\")\n",
    "\n",
    "print(\"\\n\\n‚è∞ SUBMISSION DEADLINE: [To be announced by instructor]\")\nprint(\"üìß SUBMISSION METHOD: [To be announced by instructor]\")\n\nprint(\"\\n\\nüéâ CONGRATULATIONS!\")\nprint(\"You've built a complete data science system demonstrating mastery\")\nprint(\"of Python fundamentals and real-world business analysis skills!\")\n\nprint(\"\\nüåü This capstone project showcases your transformation from\")\nprint(\"Python beginner to confident data scientist in just 3 weeks!\")\n\nprint(\"\\nüíº Your portfolio is now ready for data science job applications!\")\nprint(\"üöÄ Go forth and change the world with data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_congratulations",
   "metadata": {},
   "source": [
    "## üéì Final Congratulations!\n",
    "\n",
    "**You've reached the culmination of your Python fundamentals journey!**\n",
    "\n",
    "### üèÜ What This Project Represents\n",
    "This capstone project is more than just an assignment - it's proof of your incredible transformation:\n",
    "\n",
    "- **From Beginner to Professional**: You can now build production-quality data analysis systems\n",
    "- **Technical Mastery**: You've demonstrated command of all essential Python concepts\n",
    "- **Business Acumen**: You understand how to translate data into actionable insights\n",
    "- **Real-World Readiness**: You're prepared for actual data science roles\n",
    "\n",
    "### üåü What Makes You Special\n",
    "Unlike many who learn programming syntax, you've developed:\n",
    "- **Data intuition** - You know how to ask the right questions\n",
    "- **Quality mindset** - You validate data and handle edge cases\n",
    "- **Business focus** - You create value, not just technical complexity\n",
    "- **Communication skills** - You present insights that drive decisions\n",
    "\n",
    "### üíº Your Career Impact\n",
    "This project gives you:\n",
    "- **Portfolio-ready work** for job applications\n",
    "- **Interview talking points** about real projects you've built\n",
    "- **Confidence** to tackle any data science challenge\n",
    "- **Foundation** for advanced topics and specialization\n",
    "\n",
    "### üöÄ Next Steps\n",
    "1. **Complete this project** with excellence - it's your calling card\n",
    "2. **Add to your portfolio** - employers want to see real work\n",
    "3. **Continue learning** - build on this solid foundation\n",
    "4. **Network actively** - share your journey and help others\n",
    "5. **Apply confidently** - you have the skills companies need\n",
    "\n",
    "### üíù A Personal Message\n",
    "Three weeks ago, you couldn't write a simple Python program. Today, you're building sophisticated data analysis systems that solve real business problems.\n",
    "\n",
    "This transformation isn't just about learning code - you've developed a new way of thinking about problems, data, and solutions. You approach challenges systematically, validate your assumptions, and communicate insights clearly.\n",
    "\n",
    "You've proven that with dedication, good instruction, and hands-on practice, anyone can become a data scientist. You should be incredibly proud of what you've accomplished.\n",
    "\n",
    "### üéä The Future is Yours\n",
    "Data science is one of the most impactful fields in technology. With your skills, you can:\n",
    "- Help healthcare systems save lives through better analysis\n",
    "- Enable businesses to make smarter, data-driven decisions\n",
    "- Fight climate change with environmental data insights\n",
    "- Advance scientific research across all domains\n",
    "- Build the intelligent systems of tomorrow\n",
    "\n",
    "**The world needs thoughtful, skilled data scientists like you.**\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Final Words\n",
    "\n",
    "*\"You didn't just complete a course - you transformed your career potential. You're not just Python programmers now - you're data scientists, problem solvers, and future leaders in one of the world's most important fields.\"*\n",
    "\n",
    "*\"Go forth with confidence, curiosity, and compassion. Use your new superpowers to make the world a better place through data.\"*\n",
    "\n",
    "**üéâ Congratulations, Data Scientist! Your adventure is just beginning! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "*From all of us who believe in the power of education and the potential of every student - thank you for letting us be part of your journey. Now go change the world!* üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}