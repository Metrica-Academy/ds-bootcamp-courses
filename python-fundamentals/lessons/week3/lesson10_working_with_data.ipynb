{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Lesson 10: Working with Data\n",
    "\n",
    "**Session:** Week 3, Thursday (2 hours)  \n",
    "**Learning Objectives:**\n",
    "- Apply Python fundamentals to real data analysis\n",
    "- Clean and process messy data using core Python\n",
    "- Perform basic statistical analysis without libraries\n",
    "- Build data processing pipelines\n",
    "- Prepare for advanced data science tools (pandas preview)\n",
    "- Understand data formats and structures common in data science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome",
   "metadata": {},
   "source": [
    "## üéâ From Fundamentals to Data Science!\n",
    "\n",
    "Today we bridge the gap between **Python fundamentals** and **real data science work**!\n",
    "\n",
    "### Your Journey So Far üó∫Ô∏è\n",
    "- **Week 1**: Variables, strings, lists, dictionaries (the building blocks)\n",
    "- **Week 2**: Conditionals, loops, functions (the logic and organization)\n",
    "- **Week 3 (Tuesday)**: File I/O and error handling (working with external data)\n",
    "- **Today**: Putting it all together for **real data analysis**! üìä\n",
    "\n",
    "### What Makes This Different? ü§î\n",
    "Instead of clean, perfect data, we'll work with:\n",
    "- **Messy** data that needs cleaning\n",
    "- **Real-world** datasets with missing values\n",
    "- **Complex** structures requiring careful processing\n",
    "- **Large** amounts of information to summarize\n",
    "\n",
    "**Today you become a data scientist!** üî¨‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_reality",
   "metadata": {},
   "source": [
    "## The Reality of Data Science üìä\n",
    "\n",
    "### The Data Science Pipeline\n",
    "80% of a data scientist's time is spent on:\n",
    "1. **Data Collection** - Getting data from various sources\n",
    "2. **Data Cleaning** - Fixing inconsistencies and errors\n",
    "3. **Data Transformation** - Converting to usable formats\n",
    "4. **Exploratory Analysis** - Understanding patterns\n",
    "5. **Visualization** - Creating insights people can understand\n",
    "\n",
    "Only 20% is the \"sexy\" machine learning part!\n",
    "\n",
    "### Today's Focus: The Foundation Skills\n",
    "We'll use **pure Python** (no pandas yet!) to master the fundamentals that make you a strong data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restaurant_analogy",
   "metadata": {},
   "source": [
    "## The Restaurant Kitchen Analogy üë®‚Äçüç≥\n",
    "\n",
    "### Data Science is Like Running a Restaurant Kitchen\n",
    "\n",
    "**Raw Data = Fresh Ingredients**\n",
    "- Sometimes perfect (like clean CSV files)\n",
    "- Often messy (missing values, wrong formats, duplicates)\n",
    "- Needs inspection and preparation before use\n",
    "\n",
    "**Data Cleaning = Prep Work**\n",
    "- Wash vegetables (remove invalid data)\n",
    "- Cut ingredients uniformly (standardize formats)\n",
    "- Remove spoiled parts (handle missing values)\n",
    "- Organize ingredients by type (group and sort)\n",
    "\n",
    "**Data Analysis = Cooking Process**\n",
    "- Combine ingredients thoughtfully (merge datasets)\n",
    "- Apply heat and seasoning (mathematical operations)\n",
    "- Taste and adjust (iterative analysis)\n",
    "- Present beautifully (visualization and reporting)\n",
    "\n",
    "**Good Data Scientists** = **Master Chefs**\n",
    "- Know their ingredients (understand data)\n",
    "- Have systematic processes (reproducible workflows)\n",
    "- Handle problems gracefully (error handling)\n",
    "- Create something valuable from raw materials\n",
    "\n",
    "**Let's become data chefs!** üë®‚Äçüç≥üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_chef_intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to the Data Kitchen!\nimport json\nimport csv\nfrom datetime import datetime\nimport random\n\nprint(\"üë®‚Äçüç≥ Welcome to the Data Science Kitchen!\")\nprint(\"Today's Menu:\")\nprint(\"ü•ó Data Cleaning Techniques\")\nprint(\"üç≤ Statistical Analysis Methods\")\nprint(\"üßÑ Data Processing Workflows\")\nprint(\"üç∞ Insight Generation\")\nprint(\"\\nLet's start cooking with data! üìä\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real_data_intro",
   "metadata": {},
   "source": [
    "## Working with Real, Messy Data üßπ\n",
    "\n",
    "Let's create some realistic, messy data to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_messy_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic, messy sales data\nprint(\"=== Creating Realistic Sales Dataset ===\")\n\n# Simulate messy data like you'd find in the real world\nmessy_sales_data = [\n    # Some good records\n    \"2024-01-15,John Smith,Laptop,1,999.99,Electronics\",\n    \"2024-01-16,Sarah Johnson,Mouse,2,29.99,Electronics\", \n    \n    # Missing data\n    \"2024-01-17,,Keyboard,1,89.99,Electronics\",  # Missing customer name\n    \"2024-01-18,Mike Brown,,1,149.99,Electronics\",  # Missing product\n    \n    # Inconsistent formatting\n    \"2024-01-19,  Lisa Davis  ,smartphone,1,699.99,  ELECTRONICS  \",  # Extra spaces\n    \"01/20/2024,Bob Wilson,tablet,1,399.99,electronics\",  # Different date format\n    \n    # Data quality issues\n    \"2024-01-21,Emma White,Monitor,0,299.99,Electronics\",  # Zero quantity\n    \"2024-01-22,Tom Garcia,Headphones,-1,149.99,Electronics\",  # Negative quantity\n    \"2024-01-23,Anna Miller,Printer,1,-199.99,Electronics\",  # Negative price\n    \n    # More inconsistencies\n    \"2024-01-24,DAVID CLARK,webcam,2,79.99,Tech\",  # Different category name\n    \"2024-01-25,mary.johnson@email.com,Speaker,1,89.99,Electronics\",  # Email as name\n    \n    # Empty or malformed records\n    \"\",  # Completely empty\n    \"2024-01-26\",  # Incomplete record\n    \"2024-01-27,James Wilson,Gaming Mouse,1,59.99,Electronics,Extra Field\",  # Extra field\n]\n\n# Save the messy data\nwith open('messy_sales.txt', 'w') as file:\n    for record in messy_sales_data:\n        file.write(record + '\\n')\n\nprint(f\"‚úÖ Created messy dataset with {len(messy_sales_data)} records\")\nprint(\"üìä This data has typical real-world problems:\")\nprint(\"   ‚Ä¢ Missing values\")\nprint(\"   ‚Ä¢ Inconsistent formatting\")\nprint(\"   ‚Ä¢ Invalid data (negative quantities/prices)\")\nprint(\"   ‚Ä¢ Extra whitespace\")\nprint(\"   ‚Ä¢ Different date formats\")\nprint(\"   ‚Ä¢ Inconsistent categories\")\nprint(\"   ‚Ä¢ Malformed records\")\nprint(\"\\nüéØ Our mission: Clean and analyze this data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning",
   "metadata": {},
   "source": [
    "## Data Cleaning: The Foundation of Good Analysis üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_cleaning_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Pipeline\nprint(\"=== Data Cleaning Pipeline ===\")\n\ndef clean_sales_data(filename='messy_sales.txt'):\n    \"\"\"\n    Clean messy sales data using core Python skills\n    \n    Returns: list of cleaned dictionaries\n    \"\"\"\n    cleaned_records = []\n    errors_log = []\n    \n    try:\n        with open(filename, 'r') as file:\n            for line_num, line in enumerate(file, 1):\n                # Skip empty lines\n                if not line.strip():\n                    errors_log.append(f\"Line {line_num}: Empty line skipped\")\n                    continue\n                \n                # Split by comma\n                fields = [field.strip() for field in line.strip().split(',')]\n                \n                # Check if we have the expected number of fields\n                if len(fields) < 6:\n                    errors_log.append(f\"Line {line_num}: Incomplete record - {len(fields)} fields\")\n                    continue\n                elif len(fields) > 6:\n                    errors_log.append(f\"Line {line_num}: Extra fields found, keeping first 6\")\n                    fields = fields[:6]  # Keep only first 6 fields\n                \n                # Extract fields\n                date_str, customer_name, product, quantity_str, price_str, category = fields\n                \n                # Clean and validate each field\n                record = {}\n                record_valid = True\n                \n                # 1. Clean date\n                if date_str:\n                    # Handle different date formats\n                    if '/' in date_str:  # MM/DD/YYYY format\n                        try:\n                            month, day, year = date_str.split('/')\n                            record['date'] = f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n                        except:\n                            errors_log.append(f\"Line {line_num}: Invalid date format: {date_str}\")\n                            record_valid = False\n                    else:  # Assume YYYY-MM-DD format\n                        record['date'] = date_str\n                else:\n                    errors_log.append(f\"Line {line_num}: Missing date\")\n                    record_valid = False\n                \n                # 2. Clean customer name\n                if customer_name and '@' not in customer_name:  # Skip email addresses\n                    # Standardize name format (Title Case)\n                    record['customer_name'] = customer_name.title().strip()\n                else:\n                    errors_log.append(f\"Line {line_num}: Invalid customer name: {customer_name}\")\n                    record_valid = False\n                \n                # 3. Clean product name\n                if product:\n                    record['product'] = product.title().strip()\n                else:\n                    errors_log.append(f\"Line {line_num}: Missing product name\")\n                    record_valid = False\n                \n                # 4. Clean quantity (must be positive integer)\n                try:\n                    quantity = int(quantity_str)\n                    if quantity > 0:\n                        record['quantity'] = quantity\n                    else:\n                        errors_log.append(f\"Line {line_num}: Invalid quantity: {quantity}\")\n                        record_valid = False\n                except ValueError:\n                    errors_log.append(f\"Line {line_num}: Non-numeric quantity: {quantity_str}\")\n                    record_valid = False\n                \n                # 5. Clean price (must be positive float)\n                try:\n                    price = float(price_str)\n                    if price > 0:\n                        record['price'] = price\n                    else:\n                        errors_log.append(f\"Line {line_num}: Invalid price: {price}\")\n                        record_valid = False\n                except ValueError:\n                    errors_log.append(f\"Line {line_num}: Non-numeric price: {price_str}\")\n                    record_valid = False\n                \n                # 6. Clean category (standardize)\n                if category:\n                    # Standardize category names\n                    clean_category = category.upper().strip()\n                    if clean_category in ['ELECTRONICS', 'TECH']:\n                        record['category'] = 'Electronics'\n                    else:\n                        record['category'] = clean_category.title()\n                else:\n                    record['category'] = 'Unknown'\n                \n                # Add calculated field\n                if record_valid and 'quantity' in record and 'price' in record:\n                    record['total'] = round(record['quantity'] * record['price'], 2)\n                \n                # Only add valid records\n                if record_valid:\n                    cleaned_records.append(record)\n                \n    except FileNotFoundError:\n        print(f\"‚ùå File '{filename}' not found!\")\n        return [], []\n    except Exception as e:\n        print(f\"‚ùå Error cleaning data: {e}\")\n        return [], []\n    \n    return cleaned_records, errors_log\n\n# Clean the data\nclean_data, cleaning_errors = clean_sales_data()\n\nprint(f\"\\nüìä Data Cleaning Results:\")\nprint(f\"‚úÖ Clean records: {len(clean_data)}\")\nprint(f\"‚ö†Ô∏è Issues found: {len(cleaning_errors)}\")\n\nprint(f\"\\nüßπ Cleaning Issues Log:\")\nfor error in cleaning_errors[:5]:  # Show first 5 errors\n    print(f\"   ‚Ä¢ {error}\")\nif len(cleaning_errors) > 5:\n    print(f\"   ... and {len(cleaning_errors) - 5} more issues\")\n\nprint(f\"\\n‚úÖ Sample Clean Records:\")\nfor i, record in enumerate(clean_data[:3], 1):\n    print(f\"Record {i}: {record}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_exploration",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis with Pure Python üîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistical Analysis Functions\nprint(\"=== Basic Statistical Analysis ===\")\n\ndef calculate_statistics(numbers):\n    \"\"\"\n    Calculate basic statistics for a list of numbers\n    \n    Returns: dict with mean, median, mode, std_dev, min, max\n    \"\"\"\n    if not numbers:\n        return None\n    \n    # Sort for easier calculations\n    sorted_nums = sorted(numbers)\n    n = len(sorted_nums)\n    \n    # Mean (average)\n    mean = sum(sorted_nums) / n\n    \n    # Median (middle value)\n    if n % 2 == 0:\n        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2\n    else:\n        median = sorted_nums[n//2]\n    \n    # Mode (most frequent value)\n    from collections import Counter\n    counts = Counter(numbers)\n    mode_count = max(counts.values())\n    modes = [num for num, count in counts.items() if count == mode_count]\n    mode = modes[0] if len(modes) == 1 else modes  # Single mode or multiple\n    \n    # Standard deviation (measure of spread)\n    variance = sum((x - mean) ** 2 for x in numbers) / n\n    std_dev = variance ** 0.5\n    \n    # Range\n    min_val = min(sorted_nums)\n    max_val = max(sorted_nums)\n    \n    return {\n        'count': n,\n        'mean': round(mean, 2),\n        'median': median,\n        'mode': mode,\n        'std_dev': round(std_dev, 2),\n        'min': min_val,\n        'max': max_val,\n        'range': max_val - min_val\n    }\n\ndef analyze_sales_data(data):\n    \"\"\"\n    Perform comprehensive analysis of sales data\n    \"\"\"\n    if not data:\n        print(\"‚ùå No data to analyze!\")\n        return\n    \n    print(f\"üìä Sales Data Analysis ({len(data)} records)\")\n    print(\"=\" * 50)\n    \n    # Extract numerical data\n    quantities = [record['quantity'] for record in data]\n    prices = [record['price'] for record in data]\n    totals = [record['total'] for record in data]\n    \n    # 1. Overall Statistics\n    print(\"\\nüí∞ Revenue Analysis:\")\n    total_revenue = sum(totals)\n    total_items = sum(quantities)\n    avg_order_value = total_revenue / len(data)\n    \n    print(f\"   Total Revenue: ${total_revenue:,.2f}\")\n    print(f\"   Total Items Sold: {total_items:,}\")\n    print(f\"   Number of Orders: {len(data):,}\")\n    print(f\"   Average Order Value: ${avg_order_value:.2f}\")\n    \n    # 2. Price Statistics\n    print(\"\\nüí≤ Price Analysis:\")\n    price_stats = calculate_statistics(prices)\n    print(f\"   Average Price: ${price_stats['mean']:.2f}\")\n    print(f\"   Median Price: ${price_stats['median']:.2f}\")\n    print(f\"   Price Range: ${price_stats['min']:.2f} - ${price_stats['max']:.2f}\")\n    print(f\"   Price Std Dev: ${price_stats['std_dev']:.2f}\")\n    \n    # 3. Quantity Statistics\n    print(\"\\nüì¶ Quantity Analysis:\")\n    qty_stats = calculate_statistics(quantities)\n    print(f\"   Average Quantity: {qty_stats['mean']:.1f} items\")\n    print(f\"   Median Quantity: {qty_stats['median']:.1f} items\")\n    print(f\"   Most Common Quantity: {qty_stats['mode']} items\")\n    \n    # 4. Product Analysis\n    print(\"\\nüõçÔ∏è Product Analysis:\")\n    products = {}\n    for record in data:\n        product = record['product']\n        if product not in products:\n            products[product] = {'count': 0, 'revenue': 0, 'total_qty': 0}\n        \n        products[product]['count'] += 1\n        products[product]['revenue'] += record['total']\n        products[product]['total_qty'] += record['quantity']\n    \n    # Sort by revenue\n    sorted_products = sorted(products.items(), \n                           key=lambda x: x[1]['revenue'], \n                           reverse=True)\n    \n    print(f\"   Total Unique Products: {len(products)}\")\n    print(f\"   Top 3 Products by Revenue:\")\n    for i, (product, stats) in enumerate(sorted_products[:3], 1):\n        print(f\"      {i}. {product}: ${stats['revenue']:.2f} ({stats['count']} orders)\")\n    \n    # 5. Customer Analysis\n    print(\"\\nüë• Customer Analysis:\")\n    customers = {}\n    for record in data:\n        customer = record['customer_name']\n        if customer not in customers:\n            customers[customer] = {'orders': 0, 'revenue': 0, 'items': 0}\n        \n        customers[customer]['orders'] += 1\n        customers[customer]['revenue'] += record['total']\n        customers[customer]['items'] += record['quantity']\n    \n    # Calculate customer metrics\n    customer_revenues = [stats['revenue'] for stats in customers.values()]\n    customer_orders = [stats['orders'] for stats in customers.values()]\n    \n    print(f\"   Total Unique Customers: {len(customers)}\")\n    print(f\"   Average Revenue per Customer: ${sum(customer_revenues)/len(customer_revenues):.2f}\")\n    print(f\"   Average Orders per Customer: {sum(customer_orders)/len(customer_orders):.1f}\")\n    \n    # Find top customer\n    top_customer = max(customers.items(), key=lambda x: x[1]['revenue'])\n    print(f\"   Top Customer: {top_customer[0]} (${top_customer[1]['revenue']:.2f})\")\n    \n    # 6. Category Analysis\n    print(\"\\nüè∑Ô∏è Category Analysis:\")\n    categories = {}\n    for record in data:\n        category = record['category']\n        if category not in categories:\n            categories[category] = {'count': 0, 'revenue': 0}\n        \n        categories[category]['count'] += 1\n        categories[category]['revenue'] += record['total']\n    \n    for category, stats in categories.items():\n        percentage = (stats['revenue'] / total_revenue) * 100\n        print(f\"   {category}: ${stats['revenue']:.2f} ({percentage:.1f}% of total)\")\n    \n    return {\n        'total_revenue': total_revenue,\n        'total_orders': len(data),\n        'avg_order_value': avg_order_value,\n        'products': products,\n        'customers': customers,\n        'categories': categories\n    }\n\n# Perform the analysis\nanalysis_results = analyze_sales_data(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_grouping",
   "metadata": {},
   "source": [
    "## Advanced Data Grouping and Aggregation üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grouping_aggregation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Grouping and Aggregation\nprint(\"=== Advanced Data Grouping ===\")\n\ndef group_by_field(data, field_name, aggregation_fields):\n    \"\"\"\n    Group data by a field and calculate aggregations\n    \n    Parameters:\n    - data: list of dictionaries\n    - field_name: field to group by\n    - aggregation_fields: dict {field: [operations]} e.g., {'total': ['sum', 'avg', 'count']}\n    \n    Returns: grouped results\n    \"\"\"\n    groups = {}\n    \n    # Group the data\n    for record in data:\n        group_key = record[field_name]\n        if group_key not in groups:\n            groups[group_key] = []\n        groups[group_key].append(record)\n    \n    # Calculate aggregations for each group\n    results = {}\n    for group_key, group_records in groups.items():\n        results[group_key] = {}\n        \n        for field, operations in aggregation_fields.items():\n            values = [record[field] for record in group_records if field in record]\n            \n            if not values:\n                continue\n                \n            for operation in operations:\n                if operation == 'sum':\n                    results[group_key][f'{field}_sum'] = sum(values)\n                elif operation == 'avg':\n                    results[group_key][f'{field}_avg'] = sum(values) / len(values)\n                elif operation == 'count':\n                    results[group_key][f'{field}_count'] = len(values)\n                elif operation == 'min':\n                    results[group_key][f'{field}_min'] = min(values)\n                elif operation == 'max':\n                    results[group_key][f'{field}_max'] = max(values)\n    \n    return results\n\n# Example 1: Sales by Product\nprint(\"\\nüõçÔ∏è Sales Analysis by Product:\")\nproduct_analysis = group_by_field(\n    clean_data, \n    'product', \n    {\n        'total': ['sum', 'avg', 'count'],\n        'quantity': ['sum', 'avg'],\n        'price': ['avg', 'min', 'max']\n    }\n)\n\nfor product, metrics in sorted(product_analysis.items(), \n                             key=lambda x: x[1]['total_sum'], \n                             reverse=True)[:5]:\n    print(f\"\\n{product}:\")\n    print(f\"  Total Revenue: ${metrics['total_sum']:.2f}\")\n    print(f\"  Orders: {metrics['total_count']}\")\n    print(f\"  Avg Order Value: ${metrics['total_avg']:.2f}\")\n    print(f\"  Total Quantity: {metrics['quantity_sum']}\")\n    print(f\"  Price Range: ${metrics['price_min']:.2f} - ${metrics['price_max']:.2f}\")\n\n# Example 2: Sales by Customer\nprint(\"\\n\\nüë• Sales Analysis by Customer:\")\ncustomer_analysis = group_by_field(\n    clean_data,\n    'customer_name',\n    {\n        'total': ['sum', 'count', 'avg'],\n        'quantity': ['sum']\n    }\n)\n\n# Find VIP customers (high value)\nvip_customers = [(customer, metrics) for customer, metrics in customer_analysis.items() \n                if metrics['total_sum'] > 500 or metrics['total_count'] > 2]\n\nprint(f\"VIP Customers ({len(vip_customers)} found):\")\nfor customer, metrics in sorted(vip_customers, key=lambda x: x[1]['total_sum'], reverse=True):\n    print(f\"  {customer}: ${metrics['total_sum']:.2f} ({metrics['total_count']} orders)\")\n\n# Example 3: Time-based Analysis (by date)\nprint(\"\\n\\nüìÖ Daily Sales Trends:\")\ndaily_analysis = group_by_field(\n    clean_data,\n    'date',\n    {\n        'total': ['sum', 'count'],\n        'quantity': ['sum']\n    }\n)\n\nprint(\"Daily Sales Summary:\")\nfor date in sorted(daily_analysis.keys()):\n    metrics = daily_analysis[date]\n    print(f\"  {date}: ${metrics['total_sum']:.2f} ({metrics['total_count']} orders, {metrics['quantity_sum']} items)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_transformation",
   "metadata": {},
   "source": [
    "## Data Transformation and Enrichment üîÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation and Enrichment\nprint(\"=== Data Transformation & Enrichment ===\")\n\ndef enrich_sales_data(data):\n    \"\"\"\n    Add calculated fields and business intelligence to sales data\n    \"\"\"\n    enriched_data = []\n    \n    # Calculate overall statistics for comparisons\n    all_totals = [record['total'] for record in data]\n    avg_order_value = sum(all_totals) / len(all_totals)\n    \n    # Customer spending analysis\n    customer_totals = {}\n    for record in data:\n        customer = record['customer_name']\n        customer_totals[customer] = customer_totals.get(customer, 0) + record['total']\n    \n    # Product popularity analysis\n    product_counts = {}\n    for record in data:\n        product = record['product']\n        product_counts[product] = product_counts.get(product, 0) + 1\n    \n    for record in data.copy():  # Make a copy to avoid modifying original\n        enriched_record = record.copy()\n        \n        # 1. Order Value Category\n        if record['total'] >= avg_order_value * 1.5:\n            enriched_record['order_value_category'] = 'High'\n        elif record['total'] >= avg_order_value * 0.7:\n            enriched_record['order_value_category'] = 'Medium'\n        else:\n            enriched_record['order_value_category'] = 'Low'\n        \n        # 2. Customer Category\n        customer_total = customer_totals[record['customer_name']]\n        if customer_total >= 1000:\n            enriched_record['customer_category'] = 'VIP'\n        elif customer_total >= 500:\n            enriched_record['customer_category'] = 'Premium'\n        else:\n            enriched_record['customer_category'] = 'Standard'\n        \n        # 3. Product Popularity\n        product_popularity = product_counts[record['product']]\n        if product_popularity >= 3:\n            enriched_record['product_popularity'] = 'Popular'\n        elif product_popularity >= 2:\n            enriched_record['product_popularity'] = 'Moderate'\n        else:\n            enriched_record['product_popularity'] = 'Niche'\n        \n        # 4. Profit Margin (simulate - assume 30% margin)\n        enriched_record['estimated_profit'] = round(record['total'] * 0.30, 2)\n        \n        # 5. Day of Week (from date)\n        try:\n            from datetime import datetime\n            date_obj = datetime.strptime(record['date'], '%Y-%m-%d')\n            enriched_record['day_of_week'] = date_obj.strftime('%A')\n            enriched_record['month'] = date_obj.strftime('%B')\n        except:\n            enriched_record['day_of_week'] = 'Unknown'\n            enriched_record['month'] = 'Unknown'\n        \n        # 6. Price per Unit\n        enriched_record['price_per_unit'] = round(record['total'] / record['quantity'], 2)\n        \n        enriched_data.append(enriched_record)\n    \n    return enriched_data\n\n# Enrich the data\nenriched_sales = enrich_sales_data(clean_data)\n\nprint(f\"‚úÖ Data enriched! Added {len(enriched_sales[0]) - len(clean_data[0])} new fields\")\n\n# Show sample enriched record\nprint(\"\\nüìä Sample Enriched Record:\")\nsample_record = enriched_sales[0]\nfor key, value in sample_record.items():\n    print(f\"   {key}: {value}\")\n\n# Analyze enriched data\nprint(\"\\n\\nüìà Enriched Data Analysis:\")\n\n# Customer category distribution\ncustomer_categories = {}\nfor record in enriched_sales:\n    cat = record['customer_category']\n    customer_categories[cat] = customer_categories.get(cat, 0) + 1\n\nprint(\"\\nüë• Customer Category Distribution:\")\nfor category, count in customer_categories.items():\n    percentage = (count / len(enriched_sales)) * 100\n    print(f\"   {category}: {count} orders ({percentage:.1f}%)\")\n\n# Order value category analysis\norder_value_categories = {}\ntotal_revenue_by_category = {}\nfor record in enriched_sales:\n    cat = record['order_value_category']\n    order_value_categories[cat] = order_value_categories.get(cat, 0) + 1\n    total_revenue_by_category[cat] = total_revenue_by_category.get(cat, 0) + record['total']\n\nprint(\"\\nüí∞ Order Value Analysis:\")\nfor category in ['High', 'Medium', 'Low']:\n    if category in order_value_categories:\n        count = order_value_categories[category]\n        revenue = total_revenue_by_category[category]\n        avg_order = revenue / count\n        print(f\"   {category} Value Orders: {count} (avg: ${avg_order:.2f})\")\n\n# Day of week analysis\nday_analysis = {}\nfor record in enriched_sales:\n    day = record['day_of_week']\n    if day not in day_analysis:\n        day_analysis[day] = {'count': 0, 'revenue': 0}\n    day_analysis[day]['count'] += 1\n    day_analysis[day]['revenue'] += record['total']\n\nprint(\"\\nüìÖ Sales by Day of Week:\")\ndays_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nfor day in days_order:\n    if day in day_analysis:\n        stats = day_analysis[day]\n        avg_per_day = stats['revenue'] / stats['count']\n        print(f\"   {day}: {stats['count']} orders, ${stats['revenue']:.2f} (avg: ${avg_per_day:.2f})\")\n\n# Product popularity insights\npopularity_analysis = {}\nfor record in enriched_sales:\n    pop = record['product_popularity']\n    popularity_analysis[pop] = popularity_analysis.get(pop, 0) + 1\n\nprint(\"\\nüèÜ Product Popularity Distribution:\")\nfor popularity, count in popularity_analysis.items():\n    percentage = (count / len(enriched_sales)) * 100\n    print(f\"   {popularity}: {count} orders ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_export",
   "metadata": {},
   "source": [
    "## Creating Data Reports and Exports üìÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_reports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reporting and Export Functions\nprint(\"=== Data Reporting & Export ===\")\n\ndef generate_executive_summary(data):\n    \"\"\"\n    Generate a high-level executive summary\n    \"\"\"\n    if not data:\n        return \"No data available for analysis.\"\n    \n    # Calculate key metrics\n    total_revenue = sum(record['total'] for record in data)\n    total_orders = len(data)\n    unique_customers = len(set(record['customer_name'] for record in data))\n    unique_products = len(set(record['product'] for record in data))\n    avg_order_value = total_revenue / total_orders\n    \n    # Find date range\n    dates = [record['date'] for record in data]\n    start_date = min(dates)\n    end_date = max(dates)\n    \n    # Find top performers\n    customer_totals = {}\n    product_totals = {}\n    \n    for record in data:\n        customer = record['customer_name']\n        product = record['product']\n        \n        customer_totals[customer] = customer_totals.get(customer, 0) + record['total']\n        product_totals[product] = product_totals.get(product, 0) + record['total']\n    \n    top_customer = max(customer_totals.items(), key=lambda x: x[1])\n    top_product = max(product_totals.items(), key=lambda x: x[1])\n    \n    # Generate summary report\n    summary = f\"\"\"\nüìä EXECUTIVE SALES SUMMARY REPORT\n{'='*50}\n\nüìÖ REPORTING PERIOD: {start_date} to {end_date}\n\nüí∞ KEY FINANCIAL METRICS:\n   ‚Ä¢ Total Revenue: ${total_revenue:,.2f}\n   ‚Ä¢ Number of Orders: {total_orders:,}\n   ‚Ä¢ Average Order Value: ${avg_order_value:.2f}\n   ‚Ä¢ Revenue per Day: ${total_revenue/len(set(dates)):.2f}\n\nüë• CUSTOMER INSIGHTS:\n   ‚Ä¢ Unique Customers: {unique_customers:,}\n   ‚Ä¢ Revenue per Customer: ${total_revenue/unique_customers:.2f}\n   ‚Ä¢ Top Customer: {top_customer[0]} (${top_customer[1]:.2f})\n   ‚Ä¢ Customer Retention Rate: {(total_orders/unique_customers):.1f} orders/customer\n\nüõçÔ∏è PRODUCT PERFORMANCE:\n   ‚Ä¢ Products Sold: {unique_products:,}\n   ‚Ä¢ Revenue per Product: ${total_revenue/unique_products:.2f}\n   ‚Ä¢ Top Product: {top_product[0]} (${top_product[1]:.2f})\n\nüìà BUSINESS HEALTH INDICATORS:\n   ‚Ä¢ Order Frequency: {total_orders/len(set(dates)):.1f} orders/day\n   ‚Ä¢ Market Diversification: {unique_products} products across {len(set(record['category'] for record in data))} categories\n   ‚Ä¢ Customer Concentration: Top customer = {(top_customer[1]/total_revenue)*100:.1f}% of revenue\n\"\"\"\n    \n    return summary\n\ndef export_to_csv(data, filename='sales_analysis_export.csv'):\n    \"\"\"\n    Export data to CSV with all fields\n    \"\"\"\n    if not data:\n        print(\"‚ùå No data to export\")\n        return False\n    \n    try:\n        with open(filename, 'w', newline='') as file:\n            # Get all possible fields\n            all_fields = set()\n            for record in data:\n                all_fields.update(record.keys())\n            \n            fieldnames = sorted(all_fields)\n            \n            writer = csv.DictWriter(file, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(data)\n        \n        print(f\"‚úÖ Data exported to '{filename}' ({len(data)} records)\")\n        return True\n        \n    except Exception as e:\n        print(f\"‚ùå Export failed: {e}\")\n        return False\n\ndef create_dashboard_data(data):\n    \"\"\"\n    Create summary data suitable for dashboards\n    \"\"\"\n    dashboard_data = {\n        'summary_metrics': {},\n        'trends': {},\n        'top_performers': {},\n        'categories': {}\n    }\n    \n    # Summary metrics\n    dashboard_data['summary_metrics'] = {\n        'total_revenue': sum(record['total'] for record in data),\n        'total_orders': len(data),\n        'unique_customers': len(set(record['customer_name'] for record in data)),\n        'unique_products': len(set(record['product'] for record in data)),\n        'avg_order_value': sum(record['total'] for record in data) / len(data)\n    }\n    \n    # Daily trends\n    daily_data = {}\n    for record in data:\n        date = record['date']\n        if date not in daily_data:\n            daily_data[date] = {'revenue': 0, 'orders': 0}\n        daily_data[date]['revenue'] += record['total']\n        daily_data[date]['orders'] += 1\n    \n    dashboard_data['trends']['daily_sales'] = daily_data\n    \n    # Top performers\n    customer_revenue = {}\n    product_revenue = {}\n    \n    for record in data:\n        customer = record['customer_name']\n        product = record['product']\n        \n        customer_revenue[customer] = customer_revenue.get(customer, 0) + record['total']\n        product_revenue[product] = product_revenue.get(product, 0) + record['total']\n    \n    # Get top 5 in each category\n    dashboard_data['top_performers'] = {\n        'customers': sorted(customer_revenue.items(), key=lambda x: x[1], reverse=True)[:5],\n        'products': sorted(product_revenue.items(), key=lambda x: x[1], reverse=True)[:5]\n    }\n    \n    # Category analysis\n    category_data = {}\n    for record in data:\n        category = record['category']\n        if category not in category_data:\n            category_data[category] = {'revenue': 0, 'orders': 0}\n        category_data[category]['revenue'] += record['total']\n        category_data[category]['orders'] += 1\n    \n    dashboard_data['categories'] = category_data\n    \n    return dashboard_data\n\n# Generate and display executive summary\nexecutive_summary = generate_executive_summary(enriched_sales)\nprint(executive_summary)\n\n# Export enriched data\nexport_success = export_to_csv(enriched_sales, 'enriched_sales_data.csv')\n\n# Create dashboard data\ndashboard_metrics = create_dashboard_data(enriched_sales)\n\n# Save dashboard data as JSON\ntry:\n    with open('sales_dashboard_data.json', 'w') as file:\n        json.dump(dashboard_metrics, file, indent=2)\n    print(\"‚úÖ Dashboard data saved to 'sales_dashboard_data.json'\")\nexcept Exception as e:\n    print(f\"‚ùå Failed to save dashboard data: {e}\")\n\n# Display dashboard preview\nprint(\"\\nüìä Dashboard Data Preview:\")\nprint(f\"üìà Summary Metrics:\")\nfor metric, value in dashboard_metrics['summary_metrics'].items():\n    if 'revenue' in metric or 'value' in metric:\n        print(f\"   {metric.replace('_', ' ').title()}: ${value:,.2f}\")\n    else:\n        print(f\"   {metric.replace('_', ' ').title()}: {value:,}\")\n\nprint(f\"\\nüèÜ Top 3 Customers:\")\nfor i, (customer, revenue) in enumerate(dashboard_metrics['top_performers']['customers'][:3], 1):\n    print(f\"   {i}. {customer}: ${revenue:.2f}\")\n\nprint(f\"\\nüõçÔ∏è Top 3 Products:\")\nfor i, (product, revenue) in enumerate(dashboard_metrics['top_performers']['products'][:3], 1):\n    print(f\"   {i}. {product}: ${revenue:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "live_coding",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Live Coding: E-commerce Analytics Pipeline\n",
    "\n",
    "Let's build a complete data analysis pipeline from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecommerce_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete E-commerce Analytics Pipeline\nprint(\"üõí E-COMMERCE ANALYTICS PIPELINE\")\nprint(\"=\" * 50)\n\nclass EcommerceAnalytics:\n    \"\"\"\n    Complete e-commerce data analysis pipeline\n    \"\"\"\n    \n    def __init__(self):\n        self.raw_data = []\n        self.clean_data = []\n        self.enriched_data = []\n        self.analysis_results = {}\n        self.errors_log = []\n    \n    def load_data(self, filename):\n        \"\"\"\n        Load raw data from file\n        \"\"\"\n        print(f\"üìÇ Loading data from '{filename}'...\")\n        \n        try:\n            with open(filename, 'r') as file:\n                for line_num, line in enumerate(file, 1):\n                    if line.strip():  # Skip empty lines\n                        self.raw_data.append({\n                            'line_number': line_num,\n                            'raw_content': line.strip()\n                        })\n            \n            print(f\"‚úÖ Loaded {len(self.raw_data)} raw records\")\n            return True\n            \n        except FileNotFoundError:\n            print(f\"‚ùå File '{filename}' not found!\")\n            return False\n        except Exception as e:\n            print(f\"‚ùå Error loading data: {e}\")\n            return False\n    \n    def clean_and_validate(self):\n        \"\"\"\n        Clean and validate the raw data\n        \"\"\"\n        print(\"\\nüßπ Cleaning and validating data...\")\n        \n        cleaned_count = 0\n        error_count = 0\n        \n        for raw_record in self.raw_data:\n            line_num = raw_record['line_number']\n            content = raw_record['raw_content']\n            \n            # Parse the line\n            fields = [field.strip() for field in content.split(',')]\n            \n            # Validate field count\n            if len(fields) < 6:\n                self.errors_log.append(f\"Line {line_num}: Too few fields ({len(fields)})\")\n                error_count += 1\n                continue\n            \n            # Extract and clean fields\n            try:\n                date_str = fields[0]\n                customer = fields[1].title().strip()\n                product = fields[2].title().strip()\n                quantity = int(fields[3])\n                price = float(fields[4])\n                category = fields[5].title().strip()\n                \n                # Validate business rules\n                if not customer or '@' in customer:\n                    raise ValueError(\"Invalid customer name\")\n                if not product:\n                    raise ValueError(\"Missing product name\")\n                if quantity <= 0:\n                    raise ValueError(f\"Invalid quantity: {quantity}\")\n                if price <= 0:\n                    raise ValueError(f\"Invalid price: {price}\")\n                \n                # Standardize date format\n                if '/' in date_str:\n                    month, day, year = date_str.split('/')\n                    clean_date = f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n                else:\n                    clean_date = date_str\n                \n                # Create clean record\n                clean_record = {\n                    'date': clean_date,\n                    'customer_name': customer,\n                    'product': product,\n                    'quantity': quantity,\n                    'price': price,\n                    'category': category,\n                    'total': round(quantity * price, 2)\n                }\n                \n                self.clean_data.append(clean_record)\n                cleaned_count += 1\n                \n            except (ValueError, IndexError) as e:\n                self.errors_log.append(f\"Line {line_num}: {str(e)}\")\n                error_count += 1\n        \n        print(f\"‚úÖ Cleaned {cleaned_count} records\")\n        print(f\"‚ö†Ô∏è Found {error_count} errors\")\n        \n        return cleaned_count > 0\n    \n    def enrich_data(self):\n        \"\"\"\n        Add calculated fields and business intelligence\n        \"\"\"\n        print(\"\\nüî¨ Enriching data with business intelligence...\")\n        \n        if not self.clean_data:\n            print(\"‚ùå No clean data to enrich!\")\n            return False\n        \n        # Calculate benchmarks\n        all_totals = [record['total'] for record in self.clean_data]\n        avg_order_value = sum(all_totals) / len(all_totals)\n        \n        # Customer analysis\n        customer_stats = {}\n        for record in self.clean_data:\n            customer = record['customer_name']\n            if customer not in customer_stats:\n                customer_stats[customer] = {'orders': 0, 'revenue': 0}\n            customer_stats[customer]['orders'] += 1\n            customer_stats[customer]['revenue'] += record['total']\n        \n        # Enrich each record\n        for record in self.clean_data:\n            enriched_record = record.copy()\n            \n            # Order value tier\n            if record['total'] >= avg_order_value * 1.5:\n                enriched_record['order_tier'] = 'High'\n            elif record['total'] >= avg_order_value * 0.8:\n                enriched_record['order_tier'] = 'Medium'\n            else:\n                enriched_record['order_tier'] = 'Low'\n            \n            # Customer tier\n            customer_revenue = customer_stats[record['customer_name']]['revenue']\n            if customer_revenue >= 1000:\n                enriched_record['customer_tier'] = 'VIP'\n            elif customer_revenue >= 500:\n                enriched_record['customer_tier'] = 'Premium'\n            else:\n                enriched_record['customer_tier'] = 'Standard'\n            \n            # Profit estimation (assume 25% margin)\n            enriched_record['estimated_profit'] = round(record['total'] * 0.25, 2)\n            \n            # Day of week\n            try:\n                from datetime import datetime\n                date_obj = datetime.strptime(record['date'], '%Y-%m-%d')\n                enriched_record['day_of_week'] = date_obj.strftime('%A')\n            except:\n                enriched_record['day_of_week'] = 'Unknown'\n            \n            self.enriched_data.append(enriched_record)\n        \n        print(f\"‚úÖ Enriched {len(self.enriched_data)} records with business intelligence\")\n        return True\n    \n    def perform_analysis(self):\n        \"\"\"\n        Perform comprehensive business analysis\n        \"\"\"\n        print(\"\\nüìä Performing comprehensive analysis...\")\n        \n        if not self.enriched_data:\n            print(\"‚ùå No enriched data to analyze!\")\n            return False\n        \n        data = self.enriched_data\n        \n        # 1. Overall metrics\n        total_revenue = sum(record['total'] for record in data)\n        total_profit = sum(record['estimated_profit'] for record in data)\n        total_orders = len(data)\n        unique_customers = len(set(record['customer_name'] for record in data))\n        \n        # 2. Customer analysis\n        customer_tiers = {}\n        for record in data:\n            tier = record['customer_tier']\n            if tier not in customer_tiers:\n                customer_tiers[tier] = {'count': 0, 'revenue': 0}\n            customer_tiers[tier]['count'] += 1\n            customer_tiers[tier]['revenue'] += record['total']\n        \n        # 3. Product performance\n        products = {}\n        for record in data:\n            product = record['product']\n            if product not in products:\n                products[product] = {'orders': 0, 'revenue': 0, 'profit': 0}\n            products[product]['orders'] += 1\n            products[product]['revenue'] += record['total']\n            products[product]['profit'] += record['estimated_profit']\n        \n        # 4. Time analysis\n        daily_sales = {}\n        for record in data:\n            date = record['date']\n            if date not in daily_sales:\n                daily_sales[date] = {'orders': 0, 'revenue': 0}\n            daily_sales[date]['orders'] += 1\n            daily_sales[date]['revenue'] += record['total']\n        \n        # Store analysis results\n        self.analysis_results = {\n            'summary': {\n                'total_revenue': total_revenue,\n                'total_profit': total_profit,\n                'total_orders': total_orders,\n                'unique_customers': unique_customers,\n                'avg_order_value': total_revenue / total_orders,\n                'profit_margin': (total_profit / total_revenue) * 100\n            },\n            'customer_tiers': customer_tiers,\n            'product_performance': products,\n            'daily_trends': daily_sales\n        }\n        \n        print(\"‚úÖ Analysis complete!\")\n        return True\n    \n    def generate_report(self):\n        \"\"\"\n        Generate comprehensive business report\n        \"\"\"\n        print(\"\\nüìÑ Generating business report...\")\n        \n        if not self.analysis_results:\n            print(\"‚ùå No analysis results to report!\")\n            return None\n        \n        results = self.analysis_results\n        \n        report = f\"\"\"\nüè™ E-COMMERCE BUSINESS INTELLIGENCE REPORT\n{'='*60}\n\nüí∞ FINANCIAL PERFORMANCE:\n   Revenue: ${results['summary']['total_revenue']:,.2f}\n   Profit: ${results['summary']['total_profit']:,.2f}\n   Profit Margin: {results['summary']['profit_margin']:.1f}%\n   \nüìä OPERATIONAL METRICS:\n   Total Orders: {results['summary']['total_orders']:,}\n   Unique Customers: {results['summary']['unique_customers']:,}\n   Average Order Value: ${results['summary']['avg_order_value']:,.2f}\n   Orders per Customer: {results['summary']['total_orders']/results['summary']['unique_customers']:.1f}\n\nüë• CUSTOMER SEGMENTATION:\n\"\"\"\n        \n        for tier, stats in results['customer_tiers'].items():\n            percentage = (stats['revenue'] / results['summary']['total_revenue']) * 100\n            report += f\"   {tier}: {stats['count']} customers, ${stats['revenue']:,.2f} ({percentage:.1f}%)\\n\"\n        \n        # Top products\n        top_products = sorted(results['product_performance'].items(), \n                            key=lambda x: x[1]['revenue'], reverse=True)[:3]\n        \n        report += \"\\nüèÜ TOP PERFORMING PRODUCTS:\\n\"\n        for i, (product, stats) in enumerate(top_products, 1):\n            report += f\"   {i}. {product}: ${stats['revenue']:,.2f} ({stats['orders']} orders)\\n\"\n        \n        # Daily trends\n        avg_daily_revenue = sum(day['revenue'] for day in results['daily_trends'].values()) / len(results['daily_trends'])\n        report += f\"\\nüìà SALES TRENDS:\\n\"\n        report += f\"   Average Daily Revenue: ${avg_daily_revenue:,.2f}\\n\"\n        report += f\"   Active Sales Days: {len(results['daily_trends'])}\\n\"\n        \n        return report\n    \n    def export_results(self):\n        \"\"\"\n        Export all results to files\n        \"\"\"\n        print(\"\\nüíæ Exporting results...\")\n        \n        exports_completed = 0\n        \n        # Export clean data\n        if self.clean_data:\n            try:\n                with open('pipeline_clean_data.csv', 'w', newline='') as file:\n                    fieldnames = list(self.clean_data[0].keys())\n                    writer = csv.DictWriter(file, fieldnames=fieldnames)\n                    writer.writeheader()\n                    writer.writerows(self.clean_data)\n                exports_completed += 1\n                print(\"‚úÖ Clean data exported to 'pipeline_clean_data.csv'\")\n            except Exception as e:\n                print(f\"‚ùå Failed to export clean data: {e}\")\n        \n        # Export enriched data\n        if self.enriched_data:\n            try:\n                with open('pipeline_enriched_data.csv', 'w', newline='') as file:\n                    fieldnames = list(self.enriched_data[0].keys())\n                    writer = csv.DictWriter(file, fieldnames=fieldnames)\n                    writer.writeheader()\n                    writer.writerows(self.enriched_data)\n                exports_completed += 1\n                print(\"‚úÖ Enriched data exported to 'pipeline_enriched_data.csv'\")\n            except Exception as e:\n                print(f\"‚ùå Failed to export enriched data: {e}\")\n        \n        # Export analysis results\n        if self.analysis_results:\n            try:\n                with open('pipeline_analysis_results.json', 'w') as file:\n                    json.dump(self.analysis_results, file, indent=2)\n                exports_completed += 1\n                print(\"‚úÖ Analysis results exported to 'pipeline_analysis_results.json'\")\n            except Exception as e:\n                print(f\"‚ùå Failed to export analysis results: {e}\")\n        \n        # Export errors log\n        if self.errors_log:\n            try:\n                with open('pipeline_errors.log', 'w') as file:\n                    for error in self.errors_log:\n                        file.write(error + '\\n')\n                exports_completed += 1\n                print(\"‚úÖ Errors log exported to 'pipeline_errors.log'\")\n            except Exception as e:\n                print(f\"‚ùå Failed to export errors log: {e}\")\n        \n        return exports_completed\n\n# Run the complete pipeline\nanalytics = EcommerceAnalytics()\n\n# Execute pipeline steps\nif analytics.load_data('messy_sales.txt'):\n    if analytics.clean_and_validate():\n        if analytics.enrich_data():\n            if analytics.perform_analysis():\n                # Generate and display report\n                business_report = analytics.generate_report()\n                if business_report:\n                    print(business_report)\n                \n                # Export all results\n                exports = analytics.export_results()\n                print(f\"\\nüìÅ Pipeline complete! {exports} files exported.\")\n\nprint(\"\\nüéâ E-commerce Analytics Pipeline finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pandas_preview",
   "metadata": {},
   "source": [
    "## Preview: What's Next with Pandas üêº\n",
    "\n",
    "You've mastered data analysis with pure Python! Here's a preview of how pandas will supercharge your workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pandas_preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview: The same analysis with pandas (for comparison)\nprint(\"üêº PANDAS PREVIEW: What You'll Learn Next\")\nprint(\"=\" * 50)\n\nprint(\"\\nüìä What you did with pure Python today:\")\nprint(\"\"\"\n# Pure Python approach (what you mastered today):\ndef analyze_sales_data(data):\n    total_revenue = 0\n    customer_groups = {}\n    \n    for record in data:\n        total_revenue += record['total']\n        customer = record['customer_name']\n        if customer not in customer_groups:\n            customer_groups[customer] = []\n        customer_groups[customer].append(record['total'])\n    \n    # Calculate averages manually\n    customer_averages = {}\n    for customer, orders in customer_groups.items():\n        customer_averages[customer] = sum(orders) / len(orders)\n    \n    return total_revenue, customer_averages\n\"\"\")\n\nprint(\"\\nüöÄ The same analysis with pandas (coming soon):\")\nprint(\"\"\"\n# Pandas approach (what you'll learn next):\nimport pandas as pd\n\n# Load data in one line\ndf = pd.read_csv('sales_data.csv')\n\n# Analysis in one line each\ntotal_revenue = df['total'].sum()\ncustomer_averages = df.groupby('customer_name')['total'].mean()\nmonthly_trends = df.groupby(df['date'].dt.month)['total'].sum()\ntop_products = df.groupby('product')['total'].sum().nlargest(5)\n\n# Instant visualizations\ndf['total'].hist()  # Histogram\ndf.groupby('category')['total'].sum().plot(kind='bar')  # Bar chart\n\"\"\")\n\nprint(\"\\nüí™ Why Your Python Fundamentals Matter:\")\nprint(\"‚úÖ You understand what pandas does 'under the hood'\")\nprint(\"‚úÖ You can debug pandas operations when they go wrong\")\nprint(\"‚úÖ You can build custom functions when pandas isn't enough\")\nprint(\"‚úÖ You appreciate pandas' power because you know the manual way\")\nprint(\"‚úÖ You can combine pandas with pure Python for complex tasks\")\n\nprint(\"\\nüéØ Key Pandas Advantages:\")\nprint(\"‚Ä¢ Handles missing data automatically\")\nprint(\"‚Ä¢ Built-in statistical functions\")\nprint(\"‚Ä¢ Easy data visualization\")\nprint(\"‚Ä¢ Optimized for large datasets\")\nprint(\"‚Ä¢ Integrates with machine learning libraries\")\n\nprint(\"\\nüî• What You'll Build Next:\")\nprint(\"‚Ä¢ Interactive dashboards\")\nprint(\"‚Ä¢ Time series analysis\")\nprint(\"‚Ä¢ Data visualization\")\nprint(\"‚Ä¢ Machine learning pipelines\")\nprint(\"‚Ä¢ Real-time data processing\")\n\nprint(\"\\nüéâ You're ready for pandas because you mastered the fundamentals!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_section",
   "metadata": {},
   "source": [
    "## üéØ In-Class Exercise: Customer Behavior Analysis (25 minutes)\n",
    "\n",
    "Apply everything you've learned to analyze customer purchase patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "customer_exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Behavior Analysis Challenge\nprint(\"üéØ CUSTOMER BEHAVIOR ANALYSIS CHALLENGE\")\nprint(\"=\" * 50)\n\n# Create more complex customer transaction data\ncustomer_transactions = [\n    \"2024-01-15,John Smith,Laptop,1,999.99,Electronics,Online,Credit Card\",\n    \"2024-01-16,John Smith,Mouse,1,29.99,Electronics,Online,Credit Card\",\n    \"2024-01-20,John Smith,Keyboard,1,89.99,Electronics,Store,Cash\",\n    \"2024-01-17,Sarah Johnson,Smartphone,1,699.99,Electronics,Online,Credit Card\",\n    \"2024-01-22,Sarah Johnson,Phone Case,2,19.99,Accessories,Online,Credit Card\",\n    \"2024-01-18,Mike Brown,Tablet,1,399.99,Electronics,Store,Debit Card\",\n    \"2024-01-25,Mike Brown,Stylus,1,49.99,Accessories,Store,Cash\",\n    \"2024-01-19,Lisa Davis,Monitor,2,299.99,Electronics,Online,Credit Card\",\n    \"2024-01-26,Lisa Davis,HDMI Cable,3,15.99,Accessories,Online,Credit Card\",\n    \"2024-01-27,Lisa Davis,Webcam,1,79.99,Electronics,Store,Debit Card\",\n]\n\n# Save the transaction data\nwith open('customer_transactions.txt', 'w') as file:\n    for transaction in customer_transactions:\n        file.write(transaction + '\\n')\n\nprint(\"‚úÖ Customer transaction data created\")\nprint(\"\\nüéØ YOUR MISSION: Build a Customer Behavior Analysis System\")\nprint(\"\\nRequired Analysis:\")\nprint(\"1. Customer Lifetime Value (CLV)\")\nprint(\"2. Purchase frequency patterns\")\nprint(\"3. Channel preferences (Online vs Store)\")\nprint(\"4. Payment method analysis\")\nprint(\"5. Cross-selling opportunities\")\nprint(\"6. Customer segmentation\")\n\nprint(\"\\nüìã TODO: Complete these functions:\")\n\ndef load_customer_data(filename='customer_transactions.txt'):\n    \"\"\"\n    TODO: Load and parse customer transaction data\n    \n    Expected fields: date, customer, product, quantity, price, category, channel, payment\n    \n    Returns: list of dictionaries with clean data\n    \"\"\"\n    # Your implementation here\n    transactions = []\n    \n    # Add your data loading and cleaning logic\n    # Remember to handle errors and validate data\n    \n    return transactions\n\ndef calculate_customer_lifetime_value(transactions):\n    \"\"\"\n    TODO: Calculate CLV for each customer\n    \n    CLV = Total Revenue per Customer\n    Also calculate:\n    - Average order value per customer\n    - Purchase frequency (orders per customer)\n    - Days since last purchase\n    \n    Returns: dict {customer: clv_metrics}\n    \"\"\"\n    # Your implementation here\n    clv_data = {}\n    \n    return clv_data\n\ndef analyze_purchase_patterns(transactions):\n    \"\"\"\n    TODO: Analyze customer purchase patterns\n    \n    Find:\n    - Most popular product combinations (what's bought together)\n    - Seasonal trends (if any)\n    - Channel preferences by customer\n    - Payment method preferences\n    \n    Returns: dict with pattern analysis\n    \"\"\"\n    # Your implementation here\n    patterns = {\n        'product_combinations': {},\n        'channel_preferences': {},\n        'payment_preferences': {}\n    }\n    \n    return patterns\n\ndef segment_customers(clv_data, patterns):\n    \"\"\"\n    TODO: Segment customers into meaningful groups\n    \n    Segments:\n    - High Value (top 20% by CLV)\n    - Frequent Buyers (above average frequency)\n    - Channel Loyal (strong preference for one channel)\n    - At Risk (long time since last purchase)\n    \n    Returns: dict {segment: [customers]}\n    \"\"\"\n    # Your implementation here\n    segments = {\n        'high_value': [],\n        'frequent_buyers': [],\n        'channel_loyal': [],\n        'at_risk': []\n    }\n    \n    return segments\n\ndef generate_customer_insights(transactions, clv_data, patterns, segments):\n    \"\"\"\n    TODO: Generate actionable business insights\n    \n    Create recommendations for:\n    - Marketing campaigns\n    - Product recommendations\n    - Customer retention strategies\n    - Cross-selling opportunities\n    \n    Returns: formatted insights report\n    \"\"\"\n    # Your implementation here\n    insights = \"\"\"\n    CUSTOMER BEHAVIOR INSIGHTS REPORT\n    ================================\n    \n    [Add your analysis here]\n    \"\"\"\n    \n    return insights\n\n# TODO: Test your implementation\nprint(\"\\nüß™ Test your functions:\")\nprint(\"1. transactions = load_customer_data()\")\nprint(\"2. clv_metrics = calculate_customer_lifetime_value(transactions)\")\nprint(\"3. patterns = analyze_purchase_patterns(transactions)\")\nprint(\"4. segments = segment_customers(clv_metrics, patterns)\")\nprint(\"5. insights = generate_customer_insights(transactions, clv_metrics, patterns, segments)\")\n\nprint(\"\\nüí° HINTS:\")\nprint(\"‚Ä¢ Use datetime to calculate days between purchases\")\nprint(\"‚Ä¢ Group transactions by customer for CLV calculations\")\nprint(\"‚Ä¢ Use Counter for finding popular combinations\")\nprint(\"‚Ä¢ Set thresholds for customer segments based on your data\")\nprint(\"‚Ä¢ Focus on actionable insights that drive business decisions\")\n\nprint(\"\\n‚è∞ You have 25 minutes - good luck! üöÄ\")\n\n# Uncomment and implement:\n# transactions = load_customer_data()\n# if transactions:\n#     clv_metrics = calculate_customer_lifetime_value(transactions)\n#     patterns = analyze_purchase_patterns(transactions)\n#     segments = segment_customers(clv_metrics, patterns)\n#     insights = generate_customer_insights(transactions, clv_metrics, patterns, segments)\n#     print(insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üìö Session Summary\n",
    "\n",
    "üéâ **Incredible!** You've transformed from Python beginner to data analyst in just 3 weeks!\n",
    "\n",
    "### ‚úÖ Data Science Skills Mastered\n",
    "- **Data Cleaning**: Handling messy, real-world data like a pro\n",
    "- **Statistical Analysis**: Computing means, medians, distributions with pure Python\n",
    "- **Data Transformation**: Enriching data with calculated fields and business intelligence\n",
    "- **Grouping & Aggregation**: Advanced data summarization techniques\n",
    "- **Pipeline Development**: Building end-to-end data processing workflows\n",
    "- **Business Intelligence**: Generating actionable insights from data\n",
    "\n",
    "### üîß Technical Tools Applied\n",
    "1. **File I/O**: Reading messy CSV data from files\n",
    "2. **Error Handling**: Graceful handling of data quality issues\n",
    "3. **Data Structures**: Using lists and dictionaries for complex data\n",
    "4. **Functions**: Organizing analysis code into reusable components\n",
    "5. **Loops & Conditionals**: Processing and filtering large datasets\n",
    "6. **String Processing**: Cleaning and standardizing text data\n",
    "\n",
    "### üë®‚Äçüç≥ Remember: The Data Chef Analogy\n",
    "- **Raw data** = Fresh ingredients (often messy and inconsistent)\n",
    "- **Data cleaning** = Prep work (washing, cutting, organizing)\n",
    "- **Analysis** = Cooking process (combining, transforming, seasoning)\n",
    "- **Reports** = Final presentation (making insights digestible)\n",
    "- **Good data scientists** = Master chefs who create value from raw materials\n",
    "\n",
    "### üí° Key Insights About Data Science\n",
    "- **80% of work is data cleaning and preparation** (not the glamorous part!)\n",
    "- **Data quality determines analysis quality** (garbage in, garbage out)\n",
    "- **Business context matters more than technical complexity**\n",
    "- **Reproducible workflows prevent errors and save time**\n",
    "- **Error handling is essential for production systems**\n",
    "\n",
    "### üè† Homework Preview\n",
    "This week's final homework will include:\n",
    "1. Complete data analysis project with real dataset\n",
    "2. Building end-to-end data processing pipeline\n",
    "3. Creating business intelligence dashboard data\n",
    "4. Writing professional data analysis report\n",
    "\n",
    "### üöÄ Next Session Preview\n",
    "Saturday we'll have our **Mini-Project Workshop** where you'll build a complete data science project from scratch!\n",
    "\n",
    "### üéØ What This Means for Your Career\n",
    "- You can now handle real data science tasks\n",
    "- You understand what pandas does \"under the hood\"\n",
    "- You can build custom solutions when standard tools aren't enough\n",
    "- You have the foundation for advanced data science libraries\n",
    "- You can communicate with other data scientists and engineers\n",
    "\n",
    "### üèÜ Professional Skills Gained\n",
    "- **Problem Decomposition**: Breaking complex analysis into manageable steps\n",
    "- **Data Quality Assessment**: Identifying and fixing data issues\n",
    "- **Business Communication**: Translating technical analysis into actionable insights\n",
    "- **Workflow Design**: Creating reproducible, maintainable analysis pipelines\n",
    "- **Error Handling**: Building robust systems that handle real-world messiness\n",
    "\n",
    "**Congratulations! You're now a data scientist who can work with real data!** üìäüéì‚ú®\n",
    "\n",
    "### üí≠ Final Thought\n",
    "*\"Data science isn't about knowing every library or algorithm. It's about understanding data, asking the right questions, and communicating insights that drive decisions. You now have these fundamental skills!\"*\n",
    "\n",
    "**Ready to build something amazing on Saturday!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}